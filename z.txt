{
    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have deployed an application named app-ckad-svcn in the default namespace. Configure a service multi-port-svcn for the application which exposes the pods at multiple ports with different protocols.


    Expose port 80 using the TCP with name http

    Expose port 53 using the UDP with name dns

    info_outline
    Solution

    The pod app-ckad-svcn is deployed in default namespace.

    To view the pod along with labels, use the following command.

    student-node ~ ➜  kubectl get pods app-ckad-svcn --show-labels 
    NAME            READY   STATUS    RESTARTS   AGE     LABELS
    app-ckad-svcn   1/1     Running   0          2m58s   app=app-ckad,scenario=multiport

    We will use those labels to create the service.

    Create a service using the following manifest. It will create a service with multiple ports expose with different protocols.

    apiVersion: v1
    kind: Service
    metadata:
      name: multi-port-svcn
      labels:
           app: app-ckad
           scenario: multiport
    spec:
      selector:
          app: app-ckad
          scenario: multiport
      ports:
      - port: 80
        targetPort: 80
        protocol: TCP
        name: http
      - port: 53
        targetPort: 53
        protocol: UDP
        name: dns

	
}


{
    SECTION: SERVICES AND NETWORKING


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Deploy a pod with name messaging-ckad04-svcn using the redis:alpine image with the label tier=msg.



    Now, Create a service messaging-service-ckad04-svcn to expose the pod messaging-ckad04-svcn application within the cluster on port 6379.


    info_outline
    Solution

    Switch to cluster1 :


    kubectl config use-context cluster1




    On student-node, use the command kubectl run messaging-ckad04-svcn --image=redis:alpine -l tier=msg



    Now run the command: kubectl expose pod messaging-ckad04-svcn --port=6379 --name messaging-service-ckad04-svcn.
	
}




{
    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have already deployed an application that consists of frontend, backend, and database pods in the app-ckad namespace. Inspect them.


    Your task is to create:
    A service frontend-ckad-svcn to expose the frontend pods outside the cluster on port 31100.

    A service backend-ckad-svcn to make backend pods to be accessible within the cluster.

    A policy database-ckad-svcn to limit access to database pods only to backend pods.

    info_outline
    Solution

    A service frontend-ckad-svcn to expose the frontend pods outside the cluster on port 31100.

    apiVersion: v1
    kind: Service
    metadata:
      name: frontend-ckad-svcn
      namespace: app-ckad
    spec:
      selector:
        app: frontend
      type: NodePort
      ports:
      - name: http
        port: 80
        targetPort: 80
        nodePort: 31100

    A service backend-ckad-svcn to make backend pods to be accessible within the cluster.

    apiVersion: v1
    kind: Service
    metadata:
      name: backend-ckad-svcn
      namespace: app-ckad
    spec:
      selector:
        app: backend
      ports:
      - name: http
        port: 80
        targetPort: 80

    A policy database-ckad-svcn to limit access of database pods only to backend pods.

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: database-ckad-svcn
      namespace: app-ckad
    spec:
      podSelector:
        matchLabels:
          app: database
      ingress:
      - from:
        - podSelector:
            matchLabels:
              app: backend

	
}

{
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create an nginx pod called nginx-resolver-ckad03-svcn using image nginx, and expose it internally at port 80 with a service called nginx-resolver-service-ckad03-svcn.


    info_outline
    Solution

    Switching to cluster1:


    kubectl config use-context cluster1



    To create a pod nginx-resolver-ckad03-svcn and expose it internally:


    student-node ~ ➜ kubectl run nginx-resolver-ckad03-svcn --image=nginx 
    student-node ~ ➜ kubectl expose pod/nginx-resolver-ckad03-svcn --name=nginx-resolver-se
}

{
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Update pod ckad06-cap-aecs in the namespace ckad05-securityctx-aecs to run as root user and with the SYS_TIME and NET_ADMIN capabilities.


    Note: Make only the necessary changes. Do not modify the name of the pod.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
        terminationMessagePath: /dev/termination-log

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml > pod-capabilities.yaml

    student-node ~ ➜  vim pod-capabilities.yaml

    student-node ~ ➜  cat pod-capabilities.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad06-cap-aecs
      namespace: ckad05-securityctx-aecs
    spec:
      containers:
      - command:
        - sleep
        - "4800"
        image: ubuntu
        name: ubuntu-sleeper
        securityContext:
          capabilities:
            add: ["SYS_TIME", "NET_ADMIN"]

    student-node ~ ➜  k replace -f pod-capabilities.yaml --force 
    pod "ckad06-cap-aecs" deleted
    pod/ckad06-cap-aecs replaced

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
            - NET_ADMIN


}

{
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a role named pod-creater in the ckad20-auth-aecs namespace, and grant only the list, create and get permissions on pods resources.


    Create a role binding named mock-user-binding in the same namespace, and assign the pod-creater role to a user named mock-user.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create ns ckad20-auth-aecs

    student-node ~ ➜ kubectl create role pod-creater --namespace=ckad20-auth-aecs --verb=list,create,get --resource=pods
    role.rbac.authorization.k8s.io/pod-creater created

    student-node ~ ➜  kubectl create rolebinding mock-user-binding --namespace=ckad20-auth-aecs --role=pod-creater --user=mock-user
    rolebinding.rbac.authorization.k8s.io/mock-user-binding created

    # Now let's validate if our role and role binding is working as expected
    student-node ~ ➜  kubectl auth can-i create pod --as mock-user
    no

    student-node ~ ✖ kubectl auth can-i create pod --as mock-user --namespace ckad20-auth-aecs
    yes

}

{
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create a ConfigMap named ckad03-config-aecs in the default namespace with below specifications:


    Name: ckad03-config-aecs

        key1=Exam, value1=utlimate-mock-ckad
        key2=Provider, value2=kodekloud

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  kubectl create configmap ckad03-config-aecs --from-literal=Exam=utlimate-mock-ckad --from-literal=Provider=kodekloud
    configmap/ckad03-config-aecs created

    student-node ~ ➜  k get cm 
    ckad03-config-aecs  kube-root-ca.crt    

    student-node ~ ➜  k get cm ckad03-config-aecs -o yaml
    apiVersion: v1
    data:
      Exam: utlimate-mock-ckad
      Provider: kodekloud
    kind: ConfigMap
    metadata:
      name: ckad03-config-aecs
      namespace: default

}

{
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster2 by running:

    kubectl config use-context cluster2


    Create a custom resource my-anime of kind Anime with the below specifications:

    Name of Anime: Death Note
    Episode Count: 37


    TIP: You may find the respective CRD with anime substring in it.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster2
    Switched to context "cluster2".

    student-node ~ ➜  kubectl get crd | grep -i anime
    animes.animes.k8s.io

    student-node ~ ➜  kubectl get crd animes.animes.k8s.io \
                     -o json \
                     | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties
    {
      "animeName": {
        "type": "string"
      },
      "episodeCount": {
        "maximum": 52,
        "minimum": 24,
        "type": "integer"
      }
    }

    student-node ~ ➜  k api-resources | grep anime
    animes                            an           animes.k8s.io/v1alpha1                 true         Anime

    student-node ~ ➜  cat << YAML | kubectl apply -f -
     apiVersion: animes.k8s.io/v1alpha1
     kind: Anime
     metadata:
       name: my-anime
     spec:
       animeName: "Death Note"
       episodeCount: 37
    YAML
    anime.animes.k8s.io/my-anime created

    student-node ~ ➜  k get an my-anime 
    NAME       AGE
    my-anime   23s

}

{
	    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a new deployment called ocean-apd in the default namespace using the image kodekloud/webapp-color:v1.
    Use the following specs for the deployment:


    1. Replica count should be 2.

    2. Set the Max Unavailable to 45% and Max Surge to 55%.

    3. Create the deployment and ensure all the pods are ready.

    4. After successful deployment, upgrade the deployment image to kodekloud/webapp-color:v2 and inspect the deployment rollout status.

    5. Check the rolling history of the deployment and on the student-node, save the current revision count number to the /opt/ocean-revision-count.txt file.

    6. Finally, perform a rollback and revert the deployment image to the older version.



    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    Use the following template to create a deployment called ocean-apd: -

    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: ocean-apd
      name: ocean-apd
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: ocean-apd
      strategy: 
       type: RollingUpdate
       rollingUpdate:
         maxUnavailable: 45%
         maxSurge: 55%
      template:
        metadata:
          labels:
            app: ocean-apd
        spec:
          containers:
          - image: kodekloud/webapp-color:v1
            name: webapp-color


    Now, create the deployment by using the kubectl create -f command in the default namespace: -

    kubectl create -f <FILE-NAME>.yaml


    After sometime, upgrade the deployment image to kodekloud/webapp-color:v2: -

    kubectl set image deploy ocean-apd webapp-color=kodekloud/webapp-color:v2


    And check out the rollout history of the deployment ocean-apd: -

    kubectl rollout history deploy ocean-apd
    deployment.apps/ocean-apd 
    REVISION  CHANGE-CAUSE
    1         <none>
    2         <none>


        NOTE: - Revision count is 2. In your lab, it could be different.


    On the student-node, store the revision count to the given file: -

    echo "2" > /opt/ocean-revision-count.txt


    In final task, rollback the deployment image to an old version: -

    kubectl rollout undo deployment ocean-apd


    Verify the image name by using the following command: -

    kubectl describe deploy ocean-apd | grep -i image


    It should be kodekloud/webapp-color:v1 image.

}

kubectl scale deploy -n <namespace> --replicas=0 --all 

k scale deploy -n <namespace> <deployment name> --replicas=SOME NUMBER





{
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Update secret/ckad07-sec-cr-aecs in the default namespace with one additional credential stating the Message of the day as given below:


    Name: ckad07-sec-cr-aecs

    Creds:
    key: motd, value: We make DevOps shine!


    Note: Make only the necessary changes. Do not modify other fields of the secret.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  k get secret/ckad07-sec-cr-aecs -o yaml > secret-motd.yaml

    student-node ~ ➜  echo 'We make DevOps shine!' | base64 
    V2UgbWFrZSBEZXZPcHMgc2hpbmUhCg==

    student-node ~ ➜  vim secret-motd.yaml

    student-node ~ ➜  cat secret-motd.yaml 
    apiVersion: v1
    data:
      eligibility: RGV2T3BzR3V5cw==
      supporters: S29kZUtsb3VkIFRlYW0=
      motd: V2UgbWFrZSBEZXZPcHMgc2hpbmUhCg==  #added
    kind: Secret
    metadata:
      name: ckad07-sec-cr-aecs
      namespace: default
    type: Opaque

    student-node ~ ➜  k apply -f secret-motd.yaml 
    secret/ckad07-sec-cr-aecs configured

    student-node ~ ➜  kubectl get secret/ckad07-sec-cr-aecs -o go-template='{{.data.motd | base64decode}}'
    We make DevOps shine!


}

{
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1


    Create a pod named ckad17-qos-aecs-3 in namespace ckad17-nqoss-aecs with image nginx and container name ckad17-qos-ctr-3-aecs.

    Define other fields such that the Pod is configured to use the Quality of Service (QoS) class of Burstable.


    Also retrieve the name and QoS class of each Pod in the namespace ckad17-nqoss-aecs in the below format and save the output to a file named qos_status_aecs in the /root directory.

    Format:

    NAME    QOS
    pod-1   qos_class
    pod-2   qos_class

    info_outline
    Solution

    student-node ~ ➜ kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad17-qos-aecs-3
      namespace: ckad17-nqoss-aecs
    spec:
      containers:
      - name: ckad17-qos-ctr-3-aecs
        image: nginx
        resources:
          limits:
            memory: "200Mi"
          requests:
            memory: "100Mi"
    EOF

    pod/ckad17-qos-aecs-3 created

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
    NAME                QOS
    ckad17-qos-aecs-1   BestEffort
    ckad17-qos-aecs-2   Guaranteed
    ckad17-qos-aecs-3   Burstable

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs


}

{
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create a ResourceQuota called ckad19-rqc-aecs in the namespace ckad19-rqc-ns-aecs and enforce a limit of one ResourceQuota for the namespace.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  kubectl create namespace ckad19-rqc-ns-aecs
    namespace/ckad19-rqc-ns-aecs created

    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: ckad19-rqc-aecs
      namespace: ckad19-rqc-ns-aecs
    spec:
      hard:
        resourcequotas: "1"
    EOF

    resourcequota/ckad19-rqc-aecs created

    student-node ~ ➜  k get resourcequotas -n ckad19-rqc-ns-aecs
    NAME              AGE   REQUEST               LIMIT
    ckad19-rqc-aecs   20s   resourcequotas: 1/1   


}

{
      SECTION: SERVICES AND NETWORKING


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    A new payment service has been introduced. Since it is a sensitive application, it is deployed in its own namespace critical-space. Inspect the resources and service created.


    You are requested to make the new application available at /pay. Create an ingress resource named ingress-ckad09-svcn for the payment application to make it available at /pay



    Identify and implement the best approach to making this application available on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

    info_outline
    Solution

    Switch to Cluster3 using the following:

    kubectl config use-context cluster3

    Solution manifest file to create a new ingress service to make the application available at /pay as follows:

    ---
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: ingress-ckad09-svcn
      namespace: critical-space
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
      rules:
      - http:
          paths:
          - path: /pay
            pathType: Prefix
            backend:
              service:
               name: pay-service
               port:
                number: 8282


}

{

      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Please use the namespace nginx-depl-svcn for the following scenario.

    Create a deployment with name nginx-ckad10-svcn using nginx image with 2 replicas. Also expose the deployment via ClusterIP service .i.e. nginx-ckad10-service-svcn on port 80. Use the label app=nginx-ckad for both resources.


    Now, create a NetworkPolicy .i.e. netpol-ckad-allow-svcn so that only pods with label criteria: allow can access the deployment and apply it.

    info_outline
    Solution

    Use the following to create the deployment and the service.

    kubectl apply -n nginx-depl-svcn -f - <<eof
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-ckad10-svcn
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx-ckad
      template:
        metadata:
          labels:
            app: nginx-ckad
        spec:
          containers:
            - name: nginx
              image: nginx
              ports:
                - containerPort: 80
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx-ckad10-service-svcn
    spec:
      selector:
        app: nginx-ckad
      ports:
        - name: http
          port: 80
          targetPort: 80
      type: ClusterIP
    eof



    To create a NetworkPolicy that only allows pods with labels criteria: allow to access the deployment, you can use the following YAML definition:

    kubectl apply -n nginx-depl-svcn -f - <<eof
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: netpol-ckad-allow-svcn
    spec:
      podSelector:
        matchLabels:
          app: nginx-ckad
      ingress:
        - from:
            - podSelector:
                matchLabels:
                  criteria: allow
          ports:
            - protocol: TCP
              port: 80
    eof


}


{

      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    For this scenario, create a Service called ckad14-svcn that routes traffic to an external IP address.


    Please note that service should listen on port 80 and be of type ExternalName. Use the external IP address 10.0.0.3.



    Create the service in the default namespace.

    info_outline
    Solution

    Create the service using the following manifest:

    apiVersion: v1
    kind: Service
    metadata:
      name: ckad14-svcn
    spec:
      type: ExternalName
      externalName: 10.0.0.3
      ports:
        - name: http
          port: 80
          targetPort: 80


}


{
      SECTION: APPLICATION OBSERVABILITY AND MAINTENANCE


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1


    A pod named ckad-nginx-pod-aom is deployed and exposed with a service ckad-nginx-service-aom, but it seems the service is not configured properly and is not selecting the correct pod.

    Make the required changes to service and ensure the endpoint is configured for service.

    info_outline
    Solution

    Check for service end point by using

    kubectl describe svc ckad-nginx-service-aom


    we can see below output as below

    kubectl describe svc ckad-nginx-service-aom
    Name:              ckad-nginx-service-aom
    Namespace:         default
    Labels:            <none>
    Annotations:       <none>
    Selector:          app=ngnix
    Type:              ClusterIP
    IP Family Policy:  SingleStack
    IP Families:       IPv4
    IP:                10.43.134.231
    IPs:               10.43.134.231
    Port:              80-80  80/TCP
    TargetPort:        80/TCP
    Endpoints:         <none>
    Session Affinity:  None
    Events:            <none>


    we can see there endpoint value as none. Let's debug this we can see selector as app=ngnix . Lets check label value in pod.

    kubectl get pod ckad-nginx-pod-aom -o json | jq -r .metadata.labels
      "app": "nginx"

    we can see here selector is mis-spelled in service. so edit service and check for endpoint value.

    kubectl get ep ckad-nginx-service-aom
    NAME                     ENDPOINTS                   AGE
    ckad-nginx-service-aom   10.42.2.4:80,10.42.2.5:80   4m44s


}

{

    SECTION: APPLICATION OBSERVABILITY AND MAINTENANCE


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1


    A pod named ckad-nginx-pod-aom is deployed and exposed with a service ckad-nginx-service-aom, but it seems the service is not configured properly and is not selecting the correct pod.

    Make the required changes to service and ensure the endpoint is configured for service.

    info_outline
    Solution

    Check for service end point by using

    kubectl describe svc ckad-nginx-service-aom


    we can see below output as below

    kubectl describe svc ckad-nginx-service-aom
    Name:              ckad-nginx-service-aom
    Namespace:         default
    Labels:            <none>
    Annotations:       <none>
    Selector:          app=ngnix
    Type:              ClusterIP
    IP Family Policy:  SingleStack
    IP Families:       IPv4
    IP:                10.43.134.231
    IPs:               10.43.134.231
    Port:              80-80  80/TCP
    TargetPort:        80/TCP
    Endpoints:         <none>
    Session Affinity:  None
    Events:            <none>


    we can see there endpoint value as none. Let's debug this we can see selector as app=ngnix . Lets check label value in pod.

    kubectl get pod ckad-nginx-pod-aom -o json | jq -r .metadata.labels
      "app": "nginx"

    we can see here selector is mis-spelled in service. so edit service and check for endpoint value.

    kubectl get ep ckad-nginx-service-aom
    NAME                     ENDPOINTS                   AGE
    ckad-nginx-service-aom   10.42.2.4:80,10.42.2.5:80   4m44s


  
}


{

    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    Create a Deployment named ckad15-depl-svcn with "two replicas" of nginx image and expose it using a service named ckad15-service-svcn.


    Please be noted that service needs to be accessed from both inside and outside the cluster (use port 30085).



    Create the service in the default namespace.

    info_outline
    Solution

    The following manifest can be used to create an deployment ckad15-depl-svcn with nginx image and 2 replicas.

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ckad15-depl-svcn
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80



    To access from outside the cluster, we use nodeport type of service.

    apiVersion: v1
    kind: Service
    metadata:
      name: ckad15-service-svcn
    spec:
      selector:
        app: nginx
      type: NodePort
      ports:
        - name: http
          port: 80
          targetPort: 80
          nodePort: 30085


  
}


{

  (

    take the dict-key&value from the "deployment" selector field and 
      change the "service" selector key&value to the "deployment"'s
      
  )

SECTION: APPLICATION DEPLOYMENT


For this question, please set the context to cluster3 by running:

kubectl config use-context cluster3


We have deployed two applications called circle-apd and square-apd on the default namespace using the kodekloud/webapp-color:v1 and kodekloud/webapp-color:v2.

We have done all the tests and do not want circle-apd deployment to receive traffic from the foundary-svc service anymore. So, route all the traffic to another existing deployment.

Do change the service specifications to route traffic to the square-apd deployment.

You can test the application from the terminal by running the curl command with the following syntax: -

curl http://cluster3-controlplane:NODE-PORT
<!doctype html>
<title>Hello from Flask</title>
...
  <h2>
    Application Version: v2
  </h2>



As shown above, we will get the Application Version: v2 in the output.


  
}


{

  (
    SECTION: APPLICATION DEPLOYMENT


      For this question, please set the context to cluster2 by running:
      
      kubectl config use-context cluster2
      
      
      
      On the cluster2-controlplane node, a Helm chart repository is given under the /opt/ path. It contains the files that describe a set of Kubernetes resources that can be deployed as a single unit. The files have some issues. Fix those issues and deploy them with the following specifications: -
      
      1. The release name should be webapp-color-apd.
      
      2. All the resources should be deployed on the frontend-apd namespace.
      
      3. The service type should be node port.
      
      4. Scale the deployment to 3.
      
      5. Application version should be 1.20.0.
      
      NOTE: - Remember to make necessary changes in the values.yaml and Chart.yaml files according to the specifications, and, to fix the issues, inspect the template files.
      
      You can SSH into the cluster2 using ssh cluster2-controlplane command.
      
          
  )

Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the helm commands. Here are the steps: -


    First, check the given namespace; if it doesn't exist, we must create it first; otherwise, it will give an error "namespaces not found" while installing the helm chart.

To check all the namespaces in the cluster2, we would have to run the following command: -

kubectl get ns


It will list all the namespaces. If the given namespace doesn't exist, then run the following command: -

kubectl create ns frontend-apd

Now, SSH to the cluster2-controlplane node and go to the /opt/ directory. We have given the helm chart directory webapp-color-apd that contains templates, values files, and the chart file etc.

Update the values according to the given specifications as follows: -

a.) Update the value of the appVersion to 1.20.0 in the Chart.yaml file.

b.) Update the value of the replicaCount to 3 in the values.yaml file.

c.) Update the value of the type to NodePort in the values.yaml file.

These are the values we have to update.

Now, we will use the helm lint command to check the Helm chart because it can identify errors such as missing or misconfigured values, invalid YAML syntax, and deprecated APIs etc.

cd /opt/

helm lint ./webapp-color-apd/


If there is no misconfiguration, we will see the similar output: -

helm lint ./webapp-color-apd/
==> Linting ./webapp-color-apd/
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed



But in our case, there are some issues with the given templates.

    Deployment apiVersion needs to be correctly written. It should be apiVersion: apps/v1.

    In the service YAML, there is a typo in the template variable {{ .Values.service.name }} because of that, it's not able to reference the value of the name field defined in the values.yaml file for the Kubernetes service that is being created or updated.

  Now run the following command to install the helm chart in the frontend-apd namespace: -
  
  helm install webapp-color-apd -n frontend-apd ./webapp-color-apd
  
  
  Use the helm ls command to list the release deployed using helm.
  
  helm ls -n frontend-apd




  
}

# 9, 12, 13, 16, 17, 18, 19, 20


{ 9
      SECTION: APPLICATION OBSERVABILITY AND MAINTENANCE


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    A pod called kodekloud-logs-aom is running in the default namespace. It has two containers; get the logs of the sidecar container and copy them to /root/ckad-exam.aom on student-node.

    info_outline
    Solution

    Identify the containers name in kodekloud-logs-aom with

    kubectl get pods kodekloud-logs-aom -o json | jq '.spec.containers[].name'
    "ckad-exam"
    "sidecar"

    Use following command to logs of sidecar container

    kubectl logs kodekloud-logs-aom -c sidecar > /root/ckad-exam.aom


}

{ 12
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1


    We have a sample CRD at /root/ckad10-crd-aecs.yaml which should have the following validations:


    destinationName, country, and city must be string types.

    pricePerNight must be an integer between 50 and 5000.

    durationInDays must be an integer between 1 and 30.



    Update the file incorporating the above validations in a namespaced scope.

    Note: Remember to create the CRD after the required changes.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  vim ckad10-crd-aecs.yaml 

    student-node ~ ➜  cat ckad10-crd-aecs.yaml 
    apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    metadata:
      name: holidaydestinations.destinations.k8s.io
      annotations:
        "api-approved.kubernetes.io": "unapproved, experimental-only"
      labels:
        app: holiday
    spec:
      group: destinations.k8s.io
      names:
        kind: HolidayDestination
        singular: holidaydestination
        plural: holidaydestinations
        shortNames:
          - hd
      scope: Namespaced
      versions:
        - name: v1alpha1
          served: true
          storage: true
          schema:
            # schema used for validation
            openAPIV3Schema:
              type: object
              properties:
                spec:
                  type: object
                  properties:
                    destinationName:
                      type: string
                    country:
                      type: string
                    city:
                      type: string
                    pricePerNight:
                      type: integer
                      minimum: 50
                      maximum: 5000
                    durationInDays:
                      type: integer
                      minimum: 1
                      maximum: 30
                status:
                  type: object
                  properties:
                    availableRooms:
                      type: integer
                      minimum: 0
                      maximum: 1000
          # subresources for the custom resource
          subresources:
            # enables the status subresource
            status: {}

    student-node ~ ➜  k create -f ckad10-crd-aecs.yaml
    customresourcedefinition.apiextensions.k8s.io/holidaydestinations.destinations.k8s.io created


}

{ 13
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Update pod ckad06-cap-aecs in the namespace ckad05-securityctx-aecs to run as root user and with the SYS_TIME and NET_ADMIN capabilities.


    Note: Make only the necessary changes. Do not modify the name of the pod.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
        terminationMessagePath: /dev/termination-log

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml > pod-capabilities.yaml

    student-node ~ ➜  vim pod-capabilities.yaml

    student-node ~ ➜  cat pod-capabilities.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad06-cap-aecs
      namespace: ckad05-securityctx-aecs
    spec:
      containers:
      - command:
        - sleep
        - "4800"
        image: ubuntu
        name: ubuntu-sleeper
        securityContext:
          capabilities:
            add: ["SYS_TIME", "NET_ADMIN"]

    student-node ~ ➜  k replace -f pod-capabilities.yaml --force 
    pod "ckad06-cap-aecs" deleted
    pod/ckad06-cap-aecs replaced

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
            - NET_ADMIN


}

{ 16
      SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) in the samplecontroller.k8s.io group.

    This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:

        deploymentName (a string type) and replicas (an integer type with minimum value of 1 and maximum value of 5).



    It should also include a status subresource which enables retrieving and updating the status of Foo object, including the availableReplicas property, which is an integer type.
    The Foo resource should be namespace scoped.


    Note: We have provided a template /root/foo-crd-aecs.yaml for your ease. There are few issues with it so please make sure to incorporate the above requirements before deploying on cluster.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  vim foo-crd-aecs.yaml

    student-node ~ ➜  cat foo-crd-aecs.yaml 
    apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    metadata:
      name: foos.samplecontroller.k8s.io
      annotations:
        "api-approved.kubernetes.io": "unapproved, experimental-only"
    spec:
      group: samplecontroller.k8s.io
      scope: Namespaced
      names:
        kind: Foo
        plural: foos
      versions:
        - name: v1alpha1
          served: true
          storage: true
          schema:
            # schema used for validation
            openAPIV3Schema:
              type: object
              properties:
                spec:
                  # Spec for schema goes here !
                  type: object
                  properties:
                    deploymentName:
                      type: string
                    replicas:
                      type: integer
                      minimum: 1
                      maximum: 5
                status:
                  type: object
                  properties:
                    availableReplicas:
                      type: integer
          # subresources for the custom resource
          subresources:
            # enables the status subresource
            status: {}

    student-node ~ ➜  kubectl apply -f foo-crd-aecs.yaml
    customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created


}

{ 17
      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create an nginx pod called nginx-resolver-ckad03-svcn using image nginx, and expose it internally at port 80 with a service called nginx-resolver-service-ckad03-svcn.


    info_outline
    Solution

    Switching to cluster1:


    kubectl config use-context cluster1



    To create a pod nginx-resolver-ckad03-svcn and expose it internally:


    student-node ~ ➜ kubectl run nginx-resolver-ckad03-svcn --image=nginx 
    student-node ~ ➜ kubectl expose pod/nginx-resolver-ckad03-svcn --name=nginx-resolver-service-ckad03-svcn --port=80 --target-port=80 --type=ClusterIP 


}

{ 18
      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    For this scenario, create a Service called ckad14-svcn that routes traffic to an external IP address.


    Please note that service should listen on port 80 and be of type ExternalName. Use the external IP address 10.0.0.3.



    Create the service in the default namespace.

    info_outline
    Solution

    Create the service using the following manifest:

    apiVersion: v1
    kind: Service
    metadata:
      name: ckad14-svcn
    spec:
      type: ExternalName
      externalName: 10.0.0.3
      ports:
        - name: http
          port: 80
          targetPort: 80
}

{ 19
      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    Create a Deployment named ckad15-depl-svcn with "two replicas" of nginx image and expose it using a service named ckad15-service-svcn.


    Please be noted that service needs to be accessed from both inside and outside the cluster (use port 30085).



    Create the service in the default namespace.

    info_outline
    Solution

    The following manifest can be used to create an deployment ckad15-depl-svcn with nginx image and 2 replicas.

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ckad15-depl-svcn
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80



    To access from outside the cluster, we use nodeport type of service.

    apiVersion: v1
    kind: Service
    metadata:
      name: ckad15-service-svcn
    spec:
      selector:
        app: nginx
      type: NodePort
      ports:
        - name: http
          port: 80
          targetPort: 80
          nodePort: 30085


}

{ 20
      SECTION: SERVICES AND NETWORKING


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Deploy a pod with name messaging-ckad04-svcn using the redis:alpine image with the label tier=msg.



    Now, Create a service messaging-service-ckad04-svcn to expose the pod messaging-ckad04-svcn application within the cluster on port 6379.


    info_outline
    Solution

    Switch to cluster1 :


    kubectl config use-context cluster1




    On student-node, use the command kubectl run messaging-ckad04-svcn --image=redis:alpine -l tier=msg



    Now run the command: kubectl expose pod messaging-ckad04-svcn --port=6379 --name messaging-service-ckad04-svcn.

}

{
      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Please use the namespace nginx-depl-svcn for the following scenario.

    Create a deployment with name nginx-ckad10-svcn using nginx image with 2 replicas. Also expose the deployment via ClusterIP service .i.e. nginx-ckad10-service-svcn on port 80. Use the label app=nginx-ckad for both resources.


    Now, create a NetworkPolicy .i.e. netpol-ckad-allow-svcn so that only pods with label criteria: allow can access the deployment and apply it.

    info_outline
    Solution

    Use the following to create the deployment and the service.

    kubectl apply -n nginx-depl-svcn -f - <<eof
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-ckad10-svcn
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx-ckad
      template:
        metadata:
          labels:
            app: nginx-ckad
        spec:
          containers:
            - name: nginx
              image: nginx
              ports:
                - containerPort: 80
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx-ckad10-service-svcn
    spec:
      selector:
        app: nginx-ckad
      ports:
        - name: http
          port: 80
          targetPort: 80
      type: ClusterIP
    eof



    To create a NetworkPolicy that only allows pods with labels criteria: allow to access the deployment, you can use the following YAML definition:

    kubectl apply -n nginx-depl-svcn -f - <<eof
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: netpol-ckad-allow-svcn
    spec:
      podSelector:
        matchLabels:
          app: nginx-ckad
      ingress:
        - from:
            - podSelector:
                matchLabels:
                  criteria: allow
          ports:
            - protocol: TCP
              port: 80
    eof


}


{
  apiVersion: v1
  kind: Service
  metadata:
    name: ckad14-svcn
  spec:
    type: ExternalName
    externalName: 10.0.0.3
    ports:
      - name: http
        port: 80
        targetPort: 80
}

{
      SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    For this scenario, we have already deployed an application in the global-space. Inspect them and create an ingress resource with name ingress-resource-xnz to make the application available at /eat on the Ingress service. Use ingress class of nginx.

    Also, make use of following annotation fields: -

    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"


    Ingress resource comes under the namespace scoped, so don't forget to create the ingress in the global-space namespace.

    Make sure the paths select the correct backend services.

    info_outline
    Solution

    Switch to cluster3 by using the following command:

    kubectl config use-context cluster3

    To view the applications running on global-space namespace, run the following.

    cluster3-controlplane ~ ➜  kubectl get pod,svc -n global-space
    NAME                                 READY   STATUS    RESTARTS   AGE
    pod/default-backend-b46b9989-p9h28   1/1     Running   0          95s
    pod/webapp-food-dcd846f95-khhmw      1/1     Running   0          95s

    NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
    service/default-backend-service   ClusterIP   10.102.22.145   <none>        80/TCP     95s
    service/food-service              ClusterIP   10.99.255.77    <none>        8080/TCP   95s

    We have service food-service configured, this will act as backend service for /eat path respectively.

    Using Command Line

    Create an ingress resource using the following imperative command:

    kubectl create ingress ingress-resource-xnz \
      --namespace global-space \
      --rule='/eat'='food-service:8080' \
      --annotation='nginx.ingress.kubernetes.io/rewrite-target=/' \
      --annotation='nginx.ingress.kubernetes.io/ssl-redirect=false' \
      --class=nginx

    Using manifest file

    Solution manifest file to create a ingress resource as follows:

    ---
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: ingress-resource-xnz
      namespace: global-space
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        nginx.ingress.kubernetes.io/ssl-redirect: "false"
    spec:
      ingressClassName: nginx
      rules:
      - http:
          paths:
          - path: /eat
            pathType: Prefix
            backend:
              service:
               name: food-service
               port:
                number: 8080


}


{
  cat << 'EOF' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ckad08-top-secret-pod-aecs
  namespace: ckad08-tp-srt-aecs
spec:
  restartPolicy: Never
  volumes:
  - name: secret-volume
    secret:
      secretName: ckad08-dotfile-secret-aecs
  containers:
  - name: ckad08-top-scrt-ctr-aecs
    image: registry.k8s.io/busybox
    command:
    - ls
    - "-al"
    - "/etc/secret-volume"
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"
}

{
  kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt
}

{

    kubectl create secret generic db-user-pass \
    --from-literal=username=admin \
    --from-literal=password='S!B\*d$zDsb='

    student-node ~ ➜  kubectl config use-context cluster2
  Switched to context "cluster2".
  
  student-node ~ ➜  k get cm -n ckad21-auth2-aecs 
  NAME               DATA   AGE
  kube-root-ca.crt   1      3m35s
  ckad-cnfmp-aecs    2      3m35s
  
  student-node ~ ➜  kubectl create role configmap-updater --namespace=ckad21-auth2-aecs --resource=configmaps --resource-name=ckad-cnfmp-aecs --verb=update,get 
  role.rbac.authorization.k8s.io/configmap-updater created
  
  student-node ~ ➜  k get role -n ckad21-auth2-aecs configmap-updater -o yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    creationTimestamp: "2023-03-22T09:01:04Z"
    name: configmap-updater
    namespace: ckad21-auth2-aecs
    resourceVersion: "2799"
    uid: c152750a-198e-438e-9993-64b3e872c3e0
  rules:
  - apiGroups:
    - ""
    resourceNames:
    - ckad-cnfmp-aecs
    resources:
    - configmaps
    verbs:
    - update
    - get
  }
  
  {
    kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt
  }
  
  {
    Identify the containers name in kodekloud-logs-aom with
  
  kubectl get pods kodekloud-logs-aom -o json | jq '.spec.containers[].name'
  "ckad-exam"
  "sidecar"
  
  Use following command to logs of sidecar container
  
  kubectl logs kodekloud-logs-aom -c sidecar > /root/ckad-exam.aom
}
  
  
  
{
    Use the following command to get details:
  
  kubectl top nodes > /root/node-metrics
}



10

{

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".
    
    student-node ~ ➜  k get all -n ckad01-appstk-sec-aecs
    NAME                         READY   STATUS    RESTARTS   AGE
    pod/ckad01-webapp-pod-aecs   1/1     Running   0          3m13s
    pod/ckad01-db-pod-aecs       1/1     Running   0          3m13s
    
    NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
    service/ckad01-webapp-service-aecs   NodePort    10.43.190.89    <none>        8080:30080/TCP   3m13s
    service/ckad01-db-svc-aecs           ClusterIP   10.43.117.255   <none>        3306/TCP         3m13s
    
    student-node ~ ➜  kubectl create secret generic ckad01-db-scrt-aecs \
       --namespace=ckad01-appstk-sec-aecs \
       --from-literal=DB_Host=sql01 \
       --from-literal=DB_User=root \
       --from-literal=DB_Password=password123
    secret/ckad01-db-scrt-aecs created
    
    student-node ~ ➜  k get -n ckad01-appstk-sec-aecs pod ckad01-webapp-pod-aecs -o yaml > webapp-pod-sec-cfg.yaml
    
    student-node ~ ➜  vim webapp-pod-sec-cfg.yaml
    
    student-node ~ ➜  cat webapp-pod-sec-cfg.yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        name: ckad01-webapp-pod-aecs
      name: ckad01-webapp-pod-aecs
      namespace: ckad01-appstk-sec-aecs
    spec:
      containers:
      - image: kodekloud/simple-webapp-mysql
        imagePullPolicy: Always
        name: webapp
        envFrom:
        - secretRef:
            name: ckad01-db-scrt-aecs
    
    student-node ~ ➜  kubectl replace -f webapp-pod-sec-cfg.yaml --force 
    pod "ckad01-webapp-pod-aecs" deleted
    pod/ckad01-webapp-pod-aecs replaced
    
    student-node ~ ➜  kubectl exec -n ckad01-appstk-sec-aecs ckad01-webapp-pod-aecs -- printenv | egrep -w 'DB_Password=password123|DB_User=root|DB_Host=sql01'
    DB_Password=password123
    DB_User=root
    DB_Host=sql01

}

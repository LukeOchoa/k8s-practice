Random Notes for k8s

# this could be used to get the logs from a cronjob maybe
kubectl exec -n ckad-multi-containers ckad-sidecar-pod --container main-container -- cat /usr/share/nginx/html/index.html


# Ultimate sleeper command
command: ["sh", "-c", "sleep 5000;"]


# X: {
	SECTION: APPLICATION DESIGN AND BUILD
	
	
	For this question, please set the context to cluster1 by running:
	
	kubectl config use-context cluster1
	
	
	
	In the ckad-multi-containers namespaces, create a ckad-sidecar-pod pod that matches the following requirements.
	
	
	Pod has an emptyDir volume named my-vol.
	
	
	The first container named main-container, runs nginx:1.16 image. This container mounts the my-vol volume at /usr/share/nginx/html path.
	
	
	The second container named sidecar-container, runs busybox:1.28 image. This container mounts the my-vol volume at /var/log path.
	
	Every 5 seconds, this container should write the current date along with greeting message Hi I am from Sidecar container to index.html in the my-vol volume.
}



answer: {
	apiVersion: v1
	kind: Pod
	metadata:
	  namespace: ckad-multi-containers
	  name: ckad-sidecar-pod
	spec:
	  containers:
	    - image: nginx:1.16
	      name: main-container
	      resources: {}
	      ports:
	        - containerPort: 80
	      volumeMounts:
	        - name: my-vol
	          mountPath: /usr/share/nginx/html
	    - image: busybox:1.28
	      command:
	        - /bin/sh
	        - -c
	        - while true; do echo $(date -u) Hi I am from Sidecar container >> /var/log/index.html; sleep 5;done
	      name: sidecar-container
	      resources: {}
	      volumeMounts:
	        - name: my-vol
	          mountPath: /var/log
	  dnsPolicy: Default

	  # Location to write to
	  volumes:
	    - name: my-vol
	      emptyDir: {}
}















helm thingy: {
	
    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


	(
    	Our new client wants to deploy the resources through the popular Helm tool. 
		In the initial phase, our team lead wants to deploy nginx, 
			a very powerful and versatile web server software that is widely used to serve static content, reverse proxying, load balancing, 
			from the bitnami helm chart on the cluster3-controlplane node.
	)

    The chart URL and other specifications are as follows: -

    1. The chart URL link - https://charts.bitnami.com/bitnami

    2. The chart repository name should be polar.

    3. The release name should be nginx-server.

    4. All the resources should be deployed on the cd-tool-apd namespace.

	(
		Is Helm repository created?
		Is Helm chart installed?
		(
			Are resources deployed on ns?
		)
	)

	

    NOTE: - You have to perform this task from the student-node.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the helm commands. Here are the steps: -

    Add the repostiory to Helm with the following command: -

    helm repo add polar https://charts.bitnami.com/bitnami


    The helm repo add command is used to add a new chart repository to Helm and this allows us to browse and install charts from the new repository using the Helm package manager.


    Use the helm repo ls command which is used to list all the currently configured Helm chart repositories on Kubernetes cluster.

    helm repo ls 


    Search for the nginx chart in a polar chart repository as follows: -

    helm search repo polar | grep nginx



    The helm search repo command is used to search for charts in a specified Helm chart repository. Also, it allows us to browse available charts and view their details, such as their name, version, description, and maintainers.

    Before installing the chart, we have to create a namespace as given in the task description. Then we can install the nginx chart on a Kubernetes cluster.

    kubectl create ns cd-tool-apd

    helm install nginx-server polar/nginx -n cd-tool-apd


	v1
	{

		{
			- What needs to happen:
				1) add the repo to helm
				2) install the chart (with a namespace)
		}


		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm install nginx-server polar/nginx -n cd-tool-apd
		}

	};

	v2 (This is specifically for the question)
	{

		{
			- to do:
				1) add the repo to helm
				2) search for the chart you need
				3) create the namespace that is needed
				4) install the chart with the namespace
		}

		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm search repo polar | grep nginx
				3) k create ns cd-tool-apd
				4) helm install nginx-server polar/nginx -n cd-tool-apd
		}
		
	}

}


{

	*kubectl rollout undo -n dev-apd deploy webapp-apd
	k rollout undo -n dev-apd deployment/webapp-apd

	k rollout history deploy [deployment-name-goes-here]

	echo "2" > /opt/ocean-revision-count.txt

	k describe deployment ocean-apd | grep -i image

	kubectl top nodes > /root/node-metrics

	kubectl logs kodekloud-logs-aom -c sidecar > /root/ckad-exam.aom

}





























{
	WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config
NAME (release-name !!!)     NAMESPACE               REVISION        UPDATED                                     STATUS          CHART                           APP VERSION
atlanta-page-apd        atlanta-page-04         1               2023-07-12 10:18:51.122388204 +0000 UTC     deployed        atlanta-page-apd-0.1.0          1.16.0     
digi-locker-apd         digi-locker-02          1               2023-07-12 10:18:48.244000366 +0000 UTC     deployed        digi-locker-apd-0.1.0           1.16.0     
security-alpha-apd      security-alpha-01       1               2023-07-12 10:18:46.773087487 +0000 UTC     deployed        security-alpha-apd-0.1.0        1.16.0     
traefik                 kube-system             1               2023-07-12 10:00:51.717285983 +0000 UTC     deployed        traefik-20.3.1+up20.3.0         v2.9.4     
traefik-crd             kube-system             1               2023-07-12 10:00:31.013404459 +0000 UTC     deployed        traefik-crd-20.3.1+up20.3.0     v2.9.4     
web-dashboard-apd       web-dashboard-03        1               2023-07-12 10:18:49.727029014 +0000 UTC     deployed        web-dashboard-apd-0.1.0         1.16.0     


kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n atlanta-page-04 atlanta-page-apd -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n digi-locker-02 digi-locker-apd  -o json | jq -r '.spec.template.spec.containers[].image'




kubectl get deploy -n security-alpha-01 security-alpha-apd  -o json | jq -r '.spec.template.spec.containers[].image' 



    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    On the cluster1, the team has installed multiple helm charts on a different namespace. By mistake, those deployed resources include one of the vulnerable images called kodekloud/click-counter:latest. Find out the release name and uninstall it.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster1


    In this task, we will use the helm commands and jq tool. Here are the steps: -


        Run the helm ls command with -A option to list the releases deployed on all the namespaces using helm.

    helm ls -A


        We will use the jq tool to extract the image name from the deployments.

    kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


    Replace <NAMESPACE> with the namespace and <DEPLOYMENT-NAME> with the deployment name, which we get from the previous commands.

    After finding the kodekloud/click-counter:latest image, use the helm uninstall to remove the deployed chart that are using this vulnerable image.

    helm uninstall <RELEASE-NAME> -n <NAMESPACE> 

}



{ : Service Account :

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl command. Here are the steps: -


    Use the kubectl get command to list all the given resources: -

kubectl get po,deploy,sa,ns -n fusion-apd-x1df5


Here -n option stands for namespace, which is used to specify the namespace.

The above command will list all the resources from the fusion-apd-x1df5 namespace.

    Inspect the service account is used by the pods/deployment.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -oyaml



The deployment is using the default service account.


    Now, use the kubectl get command to retrieves the YAML definition of a deployment named robox-west-apd and save it into a file.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -o yaml > <FILE-NAME>.yaml


    Open a VI editor. Make the necessary changes and save it. It should look like this: -

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    global-kgh: robox-west-apd
  name: robox-west-apd
  namespace: fusion-apd-x1df5
spec:
  replicas: 3
  selector:
    matchLabels:
      global-kgh: robox-west-apd
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        global-kgh: robox-west-apd
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: robox-container
      serviceAccountName: galaxy-apd-xb12 ( <--------- )


    Now, replace the resource with the following command:

kubectl replace -f <FILE-NAME>.yaml --force


The above command will delete the existing deployment and create a new one with changes in the given namespace.



}



{ 


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


    After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.

    NOTE: - We have to perform this task on the cluster3-controlplane node.


    You can SSH into the cluster3 using ssh cluster3-controlplane command.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the kubectl and helm commands. Here are the steps: -


        Log in to the cluster3-controlplane node first and use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

    helm ls -A


    Here -A or --all-namespaces option lists all the releases of all the namespaces.


        Identify the namespace where the resources get deployed.


        Use the helm repo ls command to list the helm repositories.

    helm repo ls 


        Now, update the helm repository with the following command: -

    helm repo update lvm-crystal-apd -n crystal-apd-ns


    The above command updates the local cache of available charts from the configured chart repositories.


        The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.

    helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30



    The -l or --versions option is used to display information about all available chart versions.


        Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

	(
		helm upgrade [RELEASE] [CHART] [flags]
		(
			where [CHART] is release name & image name? == release-name/image-name?
		)
	)

    helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2


        After upgrading the chart version, you can verify it with the following command: -

    helm ls -n crystal-apd-ns


    Look under the CHART column for the chart version.


        Use the kubectl get command to check the replicas of the deployment: -

    kubectl get deploy -n crystal-apd-ns


    The available count 2 is under the AVAILABLE column.



	
}

csidrivers                    
csinodes                      
csistoragecapacities          
storageclasses                
volumeattachments

{ : Update with Kubectl : 
		(
			[kubectl] [set] [image] [-n] [type the namespace here] [deploy(short for deployments.apps)] [deployment name (use k get {-n some-namespace-goes-here} deployment.apps) (and maybe add the namespace)] [container name goes here]=[container-image-version-with-a-tag (: is the tag with fun decimal numbers after it)]
			k set image -n default deploy ocean-apd ocean-apd=kodekloud/webapp-color:v2
			metadata:
				labels:
						v: v1
						
		)

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl describe, kubectl get, kubectl set and kubectl scale commands. Here are the steps: -

    To check all the deployments in all the namespaces in the cluster2, we would have to run the following command:

kubectl get deployments -A


Inspect all the deployments.


    We can see that one of the deployment's names is results-apd and deployed on dashboard-apd namespace. Use the kubectl describe command to get detailed information of that deployment: -

kubectl describe -n dashboard-apd deploy results-apd


The output of the kubectl describe command will provide you with a detailed description of the deployment, including its name, namespace, creation time, labels, replicas, and the Docker image being used.


    In the previous command, we can see the container and image name under the Pod Template spec. Use the kubectl set command to update the image of that container as follows:

kubectl set image -n dashboard-apd deploy results-apd results-apd-container=nginx:1.23.3
k set image -n <namespace> deploy <deployment name> <Container-Name=ImageName:ImageVersion>

After running the above command, Kubernetes will automatically update the results-apd-container container with the new image, and create a new replica of the resource with the updated image.
The old replica will be deleted once the new one is up and running.


    Now, SSH to the cluster2-controlplane node and use echo command to add this new image to a file at /root/records/new-image-records.txt: -

echo "nginx:1.23.3" > /root/records/new-image-records.txt


If the records directory is absent, use the mkdir command to create this directory.


    NOTE: - To exit from any node, type exit on the terminal or press CTRL + D.


    Now, run the kubectl scale command to scale the deployment to 4:

kubectl scale deployment -n dashboard-apd results-apd --replicas=4

[k] [scale] [deployment] [-n] ["some-namespace"] [deployment name] [--replicas]=[any number 0-9?]


Cross-verify the scaled deployment by using the kubectl get command:

kubectl get deployments,pods -n dashboard-apd



}




{

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl and helm commands. Here are the steps: -


    To check all the resources in all namespaces in the cluster2, we would have to run the following command:

kubectl get all -A


It will list all the available resources of all namespaces.


We can see that the resources have used prefix called garuda in the name.

    To list all of the releases on the garuda-apps-apd namespace. Run the following command as follows: -

helm ls -n garuda-apps-apd


We can see the release names of the web and security applications, and they are deployed on the garuda-apps-apd namespace.


    Write their release names by using the echo command as follows: -

echo "garuda-secret-apd,garuda-web-apd" > /root/apps-release-names.txt


    NOTE: - Make sure you have logged in student-node.


    On the same cluster, one more testing application is running on the testing-apd namespace, which we must delete because it is consuming resources.

helm ls -n testing-apd


helm uninstall -n testing-apd image-scanner 


}














{
	student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs 
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          7m57s

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs -o yaml  > 1-pod-cm.yaml

# Using editor to modify the fields as per the requirements:
student-node ~ ➜  vim 1-pod-cm.yaml 

student-node ~ ➜  cat 1-pod-cm.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: ckad02-test-pod-aecs
  namespace: ckad02-mult-cm-cfg-aecs
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: ["/bin/sh", "-c", "while true; do env | egrep \"GREETINGS|WHO\"; sleep 10; done"]
      env:
        - name: GREETINGS
          valueFrom:
            configMapKeyRef:
              name: ckad02-config1-aecs
              key: greetings.how
        - name: WHO
          valueFrom:
            configMapKeyRef:
              name: ckad02-config2-aecs
              key: man
  restartPolicy: Always

student-node ~ ➜  k replace -f 1-pod-cm.yaml --force
pod "ckad02-test-pod-aecs" deleted
pod/ckad02-test-pod-aecs replaced

student-node ~ ➜  k get -f 1-pod-cm.yaml
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          10s

student-node ~ ➜  k logs -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs | head -2
GREETINGS=HI
WHO=HANDSOME
}




{
	kubectl rollout history deployment -n blue-apd foundary-apd --revision=3
}
32043


kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt











{

 Run the following command to change the context: -

kubectl config use-context cluster1


In this task, we will use the helm commands. Here are the steps: -


    Use the helm ls command to list the release deployed on the default namespace using helm.

helm ls -n default


    First, validate the helm chart by using the helm lint command: -

cd /root/

helm lint ./new-version


    Now, install the new version of the application by using the helm install command as follows: -

	helm install --generate-name ./new-version
	
	
	We haven't got any release name in the task, so we can generate the random name from the --generate-name option.
	
	Finally, uninstall the old version of the application by using the helm uninstall command: -
	
	helm uninstall webpage-server-01 -n default

	
}



{


	student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  k get po -n ckad13-ns-sa-aecs ckad13-nginx-pod-aecs -o yaml > ckad-pro-vol.yaml

student-node ~ ➜  cat ckad-pro-vol.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ckad13-nginx-pod-aecs
  namespace: ckad13-ns-sa-aecs
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
.
.
.
   volumeMounts:                              # Added
    - mountPath: /var/run/secrets/tokens       # Added
      name: vault-token                        # Added
.
.
.
  serviceAccount: ckad13-sa-aecs
  serviceAccountName: ckad13-sa-aecs
  volumes:
  - name: vault-token                   # Added
    projected:                          # Added
      sources:                          # Added
      - serviceAccountToken:            # Added
          path: vault-token             # Added
          expirationSeconds: 9000       # Added
          audience: vault               # Added

student-node ~ ➜  k replace -f ckad-pro-vol.yaml --force 
pod "ckad13-nginx-pod-aecs" deleted
pod/ckad13-nginx-pod-aecs replaced


}



{

    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    Create a service account named ckad23-sa-aecs in the namespace ckad23-nssa-aecs.

    Grant the service account get and list permissions to access all resources within the namespace using a 
		Role named wide-access-aecs.

    Also bind the Role to the service account using a RoleBinding named wide-access-rb-aecs, restricting 
		the access to the ckad23-nssa-aecs namespace only.


    Note: If the resources do not exist, please create them as well.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create ns ckad23-nssa-aecs
    namespace/ckad23-nssa-aecs created

    student-node ~ ➜  kubectl create serviceaccount ckad23-sa-aecs -n ckad23-nssa-aecs
    serviceaccount/ckad23-sa-aecs created

    student-node ~ ➜  kubectl create role wide-access-aecs --namespace=ckad23-nssa-aecs 
		--verb=get,list --resource=* 
    		role.rbac.authorization.k8s.io/wide-access-aecs created

    student-node ~ ➜  kubectl create rolebinding wide-access-rb-aecs \
       --role=wide-access-aecs \
       --serviceaccount=ckad23-nssa-aecs:ckad23-sa-aecs \
       --namespace=ckad23-nssa-aecs
    rolebinding.rbac.authorization.k8s.io/wide-access-rb-aecs created

		{
			apiVersion: v1
			items:
			- apiVersion: rbac.authorization.k8s.io/v1
			  kind: RoleBinding
			  metadata:
			    creationTimestamp: "2023-12-14T22:37:17Z"
			    name: wide-access-rb-aecs
			    namespace: ckad23-nssa-aecs
			    resourceVersion: "11643"
			    uid: 8dcf4380-81a6-483f-86c0-28f6a7394ae3
			  roleRef:
			    apiGroup: rbac.authorization.k8s.io
			    kind: Role
			    name: wide-access-aecs
			  subjects:
			  - kind: ServiceAccount
			    name: ckad23-sa-aecs
			    namespace: ckad23-nssa-aecs
			kind: List
			metadata:
			  resourceVersion: ""
			  selfLink: ""
		}

	(
		1) create the Namespace

		2) create the Service Account

		3) create the role

		4) create the RoleBinding
	)

	(
		Is service account created?
		Are roles created correctly?
		Is correct role defined for rolebinding?
		Is correct Service account specified for rolebinding
	)

	
}


{
	# create service account
kubectl create serviceaccount pod-viewer

# Create cluster role/role
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-viewer
rules:
- apiGroups: [""] # core API group
  resources: ["pods", "namespaces"]
  verbs: ["get", "watch", "list"]
---

# create cluster role binding
kubectl create clusterrolebinding pod-viewer \
  --clusterrole=pod-viewer \
  --serviceaccount=default:pod-viewer

# get service account secret
kubectl get secret | grep pod-viewer
pod-viewer-token-6fdcn   kubernetes.io/service-account-token   3      2m58s

# get token
kubectl describe secret pod-viewer-token-6fdcn
Name:         pod-viewer-token-6fdcn
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: pod-viewer
              kubernetes.io/service-account.uid: bbfb3c4e-2254-11ea-a26c-0242ac110009

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InBvZC12aWV3ZXItdG9rZW4tNmZkY24iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicG9kLXZpZXdlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImJiZmIzYzRlLTIyNTQtMTFlYS1hMjZjLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnBvZC12aWV3ZXIifQ.Pgco_4UwTCiOfYYS4QLwqgWnG8nry6JxoGiJCDuO4ZVDWUOkGJ3w6-8K1gGRSzWFOSB8E0l2YSQR4PB9jlc_9GYCFQ0-XNgkuiZBPvsTmKXdDvCNFz7bmg_Cua7HnACkKDbISKKyK4HMH-ShgVXDoMG5KmQQ_TCWs2E_a88COGMA543QL_BxckFowQZk19Iq8yEgSEfI9m8qfz4n6G7dQu9IpUSmVNUVB5GaEsaCIg6h_AXxDds5Ot6ngWUawvhYrPRv79zVKfAxYKwetjC291-qiIM92XZ63-YJJ3xbxPAsnCEwL_hG3P95-CNzoxJHKEfs_qa7a4hfe0k6HtHTWA
ca.crt:     1025 bytes
namespace:  7 bytes
```

Login to dashboard using the above token. you should see only pods and namespaces

[![Refer the below link][1]][1]


  [1]: https://i.stack.imgur.com/D9bDi.png
}

13, 14, 15, 16, 17, 18, 20


{ 13
    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster2 by running:

    kubectl config use-context cluster2


    Create a custom resource my-anime of kind Anime with the below specifications:

    Name of Anime: Death Note
    Episode Count: 37


    TIP: You may find the respective CRD with anime substring in it.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster2
    Switched to context "cluster2".

    student-node ~ ➜  kubectl get crd | grep -i anime
    animes.animes.k8s.io

    student-node ~ ➜  kubectl get crd animes.animes.k8s.io \
                     -o json \
                     | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties
    {
      "animeName": {
        "type": "string"
      },
      "episodeCount": {
        "maximum": 52,
        "minimum": 24,
        "type": "integer"
      }
    }

    student-node ~ ➜  k api-resources | grep anime
    animes                            an           animes.k8s.io/v1alpha1                 true         Anime

    student-node ~ ➜  cat << YAML | kubectl apply -f -
     apiVersion: animes.k8s.io/v1alpha1
     kind: Anime
     metadata:
       name: my-anime
     spec:
       animeName: "Death Note"
       episodeCount: 37
    YAML
    anime.animes.k8s.io/my-anime created

    student-node ~ ➜  k get an my-anime 
    NAME       AGE
    my-anime   23s

	(
		Use "k get crd" to see all the crds available

		Use "k get crd <name of crd> -o yaml" to see the yaml layout

		(apiVersion)
		For the apiVersion when you write your own "Custom Resource" as yaml, use the "group" yaml field
			under the "spec" field of the crd(in yaml form) to get your "apiVersion" first half

			the second half (for after the slash: my.custom.resourcd.d/the-second-half)
			use the "name" field after the "group" field(that you got the first half from)

		(kind)
		For the "kind" field, use the "kind" field under the "spec" -> "names" fields in the crd yaml file

		(spec fields)
		For the spec fields, use the things under "spec" -> "versions" -> ... -> "properties" for your spec fields if you need any of them
		
		
	)


	
}


{ 14
    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Update pod ckad06-cap-aecs in the namespace ckad05-securityctx-aecs to run as root user and with the SYS_TIME and NET_ADMIN capabilities.


    Note: Make only the necessary changes. Do not modify the name of the pod.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
        terminationMessagePath: /dev/termination-log

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml > pod-capabilities.yaml

    student-node ~ ➜  vim pod-capabilities.yaml

    student-node ~ ➜  cat pod-capabilities.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad06-cap-aecs
      namespace: ckad05-securityctx-aecs
    spec:
      containers:
      - command:
        - sleep
        - "4800"
        image: ubuntu
        name: ubuntu-sleeper
        securityContext:
          capabilities:
            add: ["SYS_TIME", "NET_ADMIN"]

    student-node ~ ➜  k replace -f pod-capabilities.yaml --force 
    pod "ckad06-cap-aecs" deleted
    pod/ckad06-cap-aecs replaced

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
            - NET_ADMIN

}

{ 15
	    kubectl config use-context cluster1


    Create a pod named ckad17-qos-aecs-3 in namespace ckad17-nqoss-aecs with image nginx and container name ckad17-qos-ctr-3-aecs.

    Define other fields such that the Pod is configured to use the Quality of Service (QoS) class of Burstable.


    Also retrieve the name and QoS class of each Pod in the namespace ckad17-nqoss-aecs in the below format and save the output to a file named qos_status_aecs in the /root directory.

    Format:

    NAME    QOS
    pod-1   qos_class
    pod-2   qos_class

    info_outline
    Solution

    student-node ~ ➜ kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad17-qos-aecs-3
      namespace: ckad17-nqoss-aecs
    spec:
      containers:
      - name: ckad17-qos-ctr-3-aecs
        image: nginx
        resources:
          limits:
            memory: "200Mi"
          requests:
            memory: "100Mi"
    EOF

    pod/ckad17-qos-aecs-3 created

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
    NAME                QOS
    ckad17-qos-aecs-1   BestEffort
    ckad17-qos-aecs-2   Guaranteed
    ckad17-qos-aecs-3   Burstable

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs


}


{ 16
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a ClusterRole named healthz-access that allows GET and POST requests to the non-resource endpoint /healthz and all subpaths.


    Bound this ClusterRole to a user healthz-user using a ClusterRoleBinding named healthz-access-binding.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create clusterrole healthz-access --non-resource-url=/healthz,/healthz/* --verb=get,post
    clusterrole.rbac.authorization.k8s.io/healthz-access created


    student-node ~ ➜  kubectl create clusterrolebinding healthz-access-binding --clusterrole=healthz-access --user=healthz-user
    clusterrolebinding.rbac.authorization.k8s.io/healthz-access-binding created


}


{ 17

	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    You are requested to create a network policy named deny-all-svcn that denies all incoming and outgoing traffic to ckad12-svcn namespace.


    Note: The namespace ckad12-svcn doesn't exist. Create the namespace before creating the Policy.

    info_outline
    Solution

    Create the namespace using the following command - kubectl create ns ckad12-svcn

    Create the network policy in the namespace ckad12-svcn using the following manifest.

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: deny-all-svcn
      namespace: ckad12-svcn
    spec:
      podSelector: {}
      policyTypes:
      - Ingress
      - Egress


	
}

{ 18
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have deployed a pod pod22-ckad-svcn in the default namespace. Create a service svc22-ckad-svcn that will expose the pod at port 6335.

    Note: Use the imperative command for the above scenario.

    info_outline
    Solution

    To use the cluster 3, switch the context using:

    kubectl config use-context cluster3

    To expose the pod pod22-ckad-svcn at port 6335, we can use the following imperative command.

    kubectl expose pod pod22-ckad-svcn --name=svc22-ckad-svcn --port=6335

    It will create a service with name svc22-ckad-svcn and pod will be exposed at port 6335.

	apiVersion: v1
	kind: Service
	metadata:
	  creationTimestamp: "2023-09-24T01:09:41Z"
	  labels:
	    run: pod22-ckad-svcn
	  name: svc22-ckad-svcn
	  namespace: default
	  resourceVersion: "20413"
	  uid: e86b50fa-20e5-4f77-8dc2-3daf91fbbc99
	spec:
	  clusterIP: 10.109.197.195
	  clusterIPs:
	  - 10.109.197.195
	  internalTrafficPolicy: Cluster
	  ipFamilies:
	  - IPv4
	  ipFamilyPolicy: SingleStack
	  ports:
	  - port: 6335
	    protocol: TCP
	    targetPort: 6335
	  selector:
	    run: pod22-ckad-svcn
	  sessionAffinity: None
	  type: ClusterIP
	status:
	  loadBalancer: {}
}


{ 20
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create an nginx pod called nginx-resolver-ckad03-svcn using image nginx, and expose it internally at port 80 with a service called nginx-resolver-service-ckad03-svcn.


    info_outline
    Solution

    Switching to cluster1:


    kubectl config use-context cluster1



    To create a pod nginx-resolver-ckad03-svcn and expose it internally:


    student-node ~ ➜ kubectl run nginx-resolver-ckad03-svcn --image=nginx 
    student-node ~ ➜ kubectl expose pod/nginx-resolver-ckad03-svcn --name=nginx-resolver-service-ckad03-svcn --port=80 --target-port=80 --type=ClusterIP 


}


{

	Run the following command to change the context: -

	kubectl config use-context cluster1
	
	
	In this task, we will use the kubectl command. Here are the steps: -
	
	The cube-alpha-apd and ruby-alpha-apd deployment has 5-5 replicas. The alpha-apd-service service now routes traffic to 10 pods in total (5 replicas on the ruby-alpha-apd deployment and 5 replicas from cube-alpha-apd deployment).
	
	Use the kubectl get command to list the following deployments: -
	
	kubectl get deploy -n alpha-ns-apd
	
	
	
	Since the service distributes traffic to all pods equally, in this case, approximately 50% of the traffic will go to cube-alpha-apd deployment.
	
	To reduce this below 40%, scale down the pods on the cube-alpha-apd deployment to the minimum to 2.
	
	kubectl scale deployment --replicas=2 cube-alpha-apd -n alpha-ns-apd
	
	
	Once this is done, only ~40% of traffic sho

}



{
		student-node ~ ➜  kubectl config use-context cluster2
	Switched to context "cluster2".
	
	student-node ~ ➜  k get po -n ckad13-ns-sa-aecs ckad13-nginx-pod-aecs -o yaml > ckad-pro-vol.yaml
	
	student-node ~ ➜  cat ckad-pro-vol.yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: ckad13-nginx-pod-aecs
	  namespace: ckad13-ns-sa-aecs
	spec:
	  containers:
	  - image: nginx
	    imagePullPolicy: Always
	    name: nginx
	.
	.
	.
	   volumeMounts:                              # Added
	    - mountPath: /var/run/secrets/tokens       # Added
	      name: vault-token                        # Added
	.
	.
	.
	  serviceAccount: ckad13-sa-aecs
	  serviceAccountName: ckad13-sa-aecs
	  volumes:
	  - name: vault-token                   # Added
	    projected:                          # Added
	      sources:                          # Added
	      - serviceAccountToken:            # Added
	          path: vault-token             # Added
	          expirationSeconds: 9000       # Added
	          audience: vault               # Added
	
	student-node ~ ➜  k replace -f ckad-pro-vol.yaml --force 
	pod "ckad13-nginx-pod-aecs" deleted
	pod/ckad13-nginx-pod-aecs replaced
}





































{

apiVersion: apps/v1
kind: Deployment

metadata:
  labels:
    type-one: blue
  name: blue-apd

spec:

  replicas: 7

  selector:
    matchLabels:
      type-one: blue
      version: v1


  template:

    metadata:
      labels:
        version: v1
        type-one: blue

    spec:
      containers:
        - image: kodekloud/webapp-color:v1
          name: blue-apd


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type-two: green
  name: green-apd
spec:
  replicas: 3
  selector:
    matchLabels:
      type-two: green
      version: v1
  template:
    metadata:
      labels:
        type-two: green
        version: v1
    spec:
      containers:
        - image: kodekloud/webapp-color:v2
          name: green-apd




apiVersion: v1
kind: Service
metadata:
  labels:
    app: route-apd-svc
  name: route-apd-svc
spec:
  type: NodePort
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    version: v1
	
}



{
	scale down deployments
	kubectl scale deploy -n <namespace> --replicas=0 --all 


	
}





{


		Run the following command to change the context: -
	
	kubectl config use-context cluster2
	
	
	In this task, we will use the kubectl command. Here are the steps: -
	
	
	    Use the kubectl get command to list all the given resources: -
	
	kubectl get po,deploy,sa,ns -n fusion-apd-x1df5
	
	
	Here -n option stands for namespace, which is used to specify the namespace.
	
	The above command will list all the resources from the fusion-apd-x1df5 namespace.
	
	    Inspect the service account is used by the pods/deployment.
	
	kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -oyaml
	
	
	
	The deployment is using the default service account.
	
	
	    Now, use the kubectl get command to retrieves the YAML definition of a deployment named robox-west-apd and save it into a file.
	
	kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -o yaml > <FILE-NAME>.yaml
	
	
	    Open a VI editor. Make the necessary changes and save it. It should look like this: -
	
	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  labels:
	    global-kgh: robox-west-apd
	  name: robox-west-apd
	  namespace: fusion-apd-x1df5
	spec:
	  replicas: 3
	  selector:
	    matchLabels:
	      global-kgh: robox-west-apd
	  strategy:
	    type: Recreate
	  template:
	    metadata:
	      labels:
	        global-kgh: robox-west-apd
	    spec:
	      containers:
	      - image: nginx
	        imagePullPolicy: Always
	        name: robox-container
	      serviceAccountName: galaxy-apd-xb12
	
	
	    Now, replace the resource with the following command:
	
	kubectl replace -f <FILE-NAME>.yaml --force
	
	
	The above command will delete the existing deployment and create a new one with changes in the given namespace.



}



{


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    stage: test02
  name: test-v2-apd
spec:
  replicas: 3
  selector:
    matchLabels:
      test-app: v1
  template:
    metadata:
      labels:
        test-app: v1
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: test-v2








(
	    For this question, please set the context to cluster2 by running:

    kubectl config use-context cluster2


    Please fix any possible mistakes in the manifest file located at /root/ckad11-obj-aecs.yaml, which contains a sample object created from a custom resource definition.

    Note: Create the object using the above manifest when done.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster2
    Switched to context "cluster2".

    student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
    error: error parsing /root/ckad11-obj-aecs.yaml: error converting YAML to JSON: yaml: line 7: mapping values are not allowed in this context

    student-node ~ ✖ vim /root/ckad11-obj-aecs.yaml

    student-node ~ ➜  cat /root/ckad11-obj-aecs.yaml
    apiVersion: apps/v1
    kind: MyCustomResource
    metadata:
      name: ckad11-my-custom-resource-aecs
    spec:
      name: John  #1 Fixed the indentation issue
      age: -10
    status:
      phase: Invalid

    student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
    error: unable to recognize "/root/ckad11-obj-aecs.yaml": no matches for kind "MyCustomResource" in version "apps/v1"

    student-node ~ ✖ kubectl api-resources | grep -i MyCustomResource
    mycustomresources                 mcr          example.com/v1                         true         MyCustomResource

    student-node ~ ➜  vim /root/ckad11-obj-aecs.yaml

    student-node ~ ➜  cat /root/ckad11-obj-aecs.yaml
    apiVersion: example.com/v1  #2 fixed the apiVersion used
    kind: MyCustomResource
    metadata:
      name: ckad11-my-custom-resource-aecs
    spec:
      name: John  #1 Fixed the indentation issue
      age: -10
    status:
      phase: Invalid

    student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
    The MyCustomResource "ckad11-my-custom-resource-aecs" is invalid: 
    * spec.age: Invalid value: -10: spec.age in body should be greater than or equal to 5
    * status.phase: Unsupported value: "Invalid": supported values: "Pending", "Running", "Completed"

    student-node ~ ✖ vim /root/ckad11-obj-aecs.yaml

    student-node ~ ➜  cat /root/ckad11-obj-aecs.yaml
    apiVersion: example.com/v1  #2 fixed the apiVersion used
    kind: MyCustomResource
    metadata:
      name: ckad11-my-custom-resource-aecs
    spec:
      name: John  #1 Fixed the indentation issue
      age: 5 #3 use the valid entries
    status:
      phase: Running #4 use the valid entries

    student-node ~ ➜  k apply -f /root/ckad11-obj-aecs.yaml
    mycustomresource.example.com/ckad11-my-custom-resource-aecs created


)


(
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) in the samplecontroller.k8s.io group.

    This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:

        deploymentName (a string type) and replicas (an integer type with minimum value of 1 and maximum value of 5).



    It should also include a status subresource which enables retrieving and updating the status of Foo object, including the availableReplicas property, which is an integer type.
    The Foo resource should be namespace scoped.


    Note: We have provided a template /root/foo-crd-aecs.yaml for your ease. There are few issues with it so please make sure to incorporate the above requirements before deploying on cluster.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  vim foo-crd-aecs.yaml

    student-node ~ ➜  cat foo-crd-aecs.yaml 
    apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    metadata:
      name: foos.samplecontroller.k8s.io
      annotations:
        "api-approved.kubernetes.io": "unapproved, experimental-only"
    spec:
      group: samplecontroller.k8s.io
      scope: Namespaced
      names:
        kind: Foo
        plural: foos
      versions:
        - name: v1alpha1
          served: true
          storage: true
          schema:
            # schema used for validation
            openAPIV3Schema:
              type: object
              properties:
                spec:
                  # Spec for schema goes here !
                  type: object
                  properties:
                    deploymentName:
                      type: string
                    replicas:
                      type: integer
                      minimum: 1
                      maximum: 5
                status:
                  type: object
                  properties:
                    availableReplicas:
                      type: integer
          # subresources for the custom resource
          subresources:
            # enables the status subresource
            status: {}

    student-node ~ ➜  kubectl apply -f foo-crd-aecs.yaml
    customresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io created


)

(
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    In the ckad18-rq-ns-aecs namespace, create a ResourceQuota called ckad18-rq-aecs. This ResourceQuota should have the following specifications for CPU and memory:

        Requests: CPU: 50m, Memory: 100Mi

        Limits: CPU: 100m, Memory: 200Mi

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  kubectl create ns ckad18-rq-ns-aecs
    namespace/ckad18-rq-ns-aecs created

    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: ckad18-rq-aecs
      namespace: ckad18-rq-ns-aecs
    spec:
      hard:
        requests.cpu: "50m"
        requests.memory: 100Mi
        limits.cpu: "100m"
        limits.memory: 200Mi
    EOF

    resourcequota/ckad18-rq-aecs created

    student-node ~ ➜  kubectl get resourcequotas -n ckad18-rq-ns-aecs 
    NAME           AGE   REQUEST                                         LIMIT
    mem-cpu-aecs   1m   requests.cpu: 0/50m, requests.memory: 0/100Mi   limits.cpu: 0/100m, limits.memory: 0/200Mi


    # Validating if the resource quotas working or not !!
    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: Pod
    metadata:
      name: quota-mem-cpu-demo
      namespace: ckad18-rq-ns-aecs
    spec:
      containers:
      - name: quota-mem-cpu-demo-ctr
        image: nginx
        resources:
          limits:
            memory: "210Mi"
            cpu: "200m"
          requests:
            memory: "100Mi"
            cpu: "50m"
    EOF

    Error from server (Forbidden): error when creating "STDIN": pods "quota-mem-cpu-demo" is forbidden: exceeded quota: mem-cpu-aecs, requested: limits.cpu=200m,limits.memory=210Mi, used: limits.cpu=0,limits.memory=0, limited: limits.cpu=100m,limits.memory=200Mi


)


(
	    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a role named pod-creater in the ckad20-auth-aecs namespace, and grant only the list, create and get permissions on pods resources.


    Create a role binding named mock-user-binding in the same namespace, and assign the pod-creater role to a user named mock-user.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create ns ckad20-auth-aecs

    student-node ~ ➜ kubectl create role pod-creater --namespace=ckad20-auth-aecs --verb=list,create,get --resource=pods
    role.rbac.authorization.k8s.io/pod-creater created

    student-node ~ ➜  kubectl create rolebinding mock-user-binding --namespace=ckad20-auth-aecs --role=pod-creater --user=mock-user
    rolebinding.rbac.authorization.k8s.io/mock-user-binding created

    # Now let's validate if our role and role binding is working as expected
    student-node ~ ➜  kubectl auth can-i create pod --as mock-user
    no

    student-node ~ ✖ kubectl auth can-i create pod --as mock-user --namespace ckad20-auth-aecs
    yes


)

(
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    Create a pod with name pod21-ckad-svcn using the nginx:alpine image in the default namespace and also expose the pod using service pod21-ckad-svcn on port 80.

    Note: Use the imperative command for above scenario.

    info_outline
    Solution

    To use the cluster 3, switch the context using:

    kubectl config use-context cluster3

    As the scenario mention to create a pod along with its service with name pod21-ckad-svcn, we can use the following imperative command.

    kubectl run pod21-ckad-svcn --image=nginx:alpine --expose --port=80

    It will create a pod and also exposes the pod at port 80
)

(
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have already deployed an ingress resource in the app-space namespace.


    But for better SEO practices, you are requested to change the URLs at which the applications are made available.


    Change the path of the video application to make it available at /stream.



    Note: Check the backend services configured for the paths in the ingress resource.

    info_outline
    Solution

    To change the path of the service, you need to edit the ingress

    Edit the ingress using kubectl edit -n app-space ingress ingress-resource-svcn.
    Edit the /watch to make it as /stream.

    spec:
      ingressClassName: nginx
      rules:
      - http:
          paths:
          - backend:
              service:
                name: wear-service
                port:
                  number: 8080
            path: /wear
            pathType: Prefix
          - backend:
              service:
                name: video-service
                port:
                  number: 8080
            path: /watch  #change path to /stream
            pathType: Prefix
    status:
      loadBalancer:
        ingress:
        - ip: 10.108.22.212


)

(

	    SECTION: SERVICES AND NETWORKING


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    This scenario is categorized into two parts. Please find them below.

    Part I:

    We have already deployed several pods in the default namespace. Create a ClusterIP service .i.e. service-3421-svcn, which should expose the pods, namely, pod-23 and pod-21, with port set to 8080 and targetport to 80.

    Part II:
    Store the pod names and their ip addresses from all namespaces at /root/pod_ips_ckad02_svcn where the output is sorted by their IPs.

    Please ensure the format as shown below:


    POD_NAME        IP_ADDR
    pod-1           ip-1
    pod-3           ip-2
    pod-2           ip-3
    ...



    info_outline
    Solution

    Switching to cluster3:


    kubectl config use-context cluster3



    The easiest way to route traffic to a specific pod is by the use of labels and selectors . List the pods along with their labels:


    student-node ~ ➜  kubectl get pods --show-labels 
    NAME     READY   STATUS    RESTARTS   AGE     LABELS
    pod-12   1/1     Running   0          5m21s   env=dev,mode=standard,type=external
    pod-34   1/1     Running   0          5m20s   env=dev,mode=standard,type=internal
    pod-43   1/1     Running   0          5m20s   env=prod,mode=exam,type=internal
    pod-23   1/1     Running   0          5m21s   env=dev,mode=exam,type=external
    pod-32   1/1     Running   0          5m20s   env=prod,mode=standard,type=internal
    pod-21   1/1     Running   0          5m20s   env=prod,mode=exam,type=external



    Looks like there are a lot of pods created to confuse us. But we are only concerned with the labels of pod-23 and pod-21.



    As we can see both the required pods have labels mode=exam,type=external in common. Let's confirm that using kubectl too:


    student-node ~ ➜  kubectl get pod -l mode=exam,type=external                                       
    NAME     READY   STATUS    RESTARTS   AGE
    pod-23   1/1     Running   0          9m18s
    pod-21   1/1     Running   0          9m17s




    Nice!! Now as we have figured out the labels, we can proceed further with the creation of the service:


    student-node ~ ➜  kubectl create service clusterip service-3421-svcn --tcp=8080:80 --dry-run=client -o yaml > service-3421-svcn.yaml




    Now modify the service definition with selectors as required before applying to k8s cluster:


    student-node ~ ➜  cat service-3421-svcn.yaml 
    apiVersion: v1
    kind: Service
    metadata:
      creationTimestamp: null
      labels:
        app: service-3421-svcn
      name: service-3421-svcn
    spec:
      ports:
      - name: 8080-80
        port: 8080
        protocol: TCP
        targetPort: 80
      selector:
        app: service-3421-svcn  # delete 
        mode: exam    # add
        type: external  # add
      type: ClusterIP
    status:
      loadBalancer: {}



    Finally let's apply the service definition:


    student-node ~ ➜  kubectl apply -f service-3421-svcn.yaml
    service/service-3421 created

    student-node ~ ➜  k get ep service-3421-svcn 
    NAME           ENDPOINTS                     AGE
    service-3421   10.42.0.15:80,10.42.0.17:80   52s




    To store all the pod name along with their IP's , we could use imperative command as shown below:


    student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP

    POD_NAME                                  IP_ADDR
    helm-install-traefik-crd-lbwzr            10.42.0.2
    local-path-provisioner-7b7dc8d6f5-d4x7t   10.42.0.3
    metrics-server-668d979685-vh7bk           10.42.0.4
    ...

    # store the output to /root/pod_ips_ckad02_svcn
    student-node ~ ➜  kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_ckad02_svcn


)

(
	    SECTION: SERVICES AND NETWORKING


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Deploy a pod with name messaging-ckad04-svcn using the redis:alpine image with the label tier=msg.



    Now, Create a service messaging-service-ckad04-svcn to expose the pod messaging-ckad04-svcn application within the cluster on port 6379.


    info_outline
    Solution

    Switch to cluster1 :


    kubectl config use-context cluster1




    On student-node, use the command kubectl run messaging-ckad04-svcn --image=redis:alpine -l tier=msg



    Now run the command: kubectl expose pod messaging-ckad04-svcn --port=6379 --name messaging-service-ckad04-svcn.

)








	
}


{

	    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a new deployment called ocean-apd in the default namespace using the image kodekloud/webapp-color:v1.
    Use the following specs for the deployment:


    1. Replica count should be 2.

    2. Set the Max Unavailable to 45% and Max Surge to 55%.

    3. Create the deployment and ensure all the pods are ready.

    4. After successful deployment, upgrade the deployment image to kodekloud/webapp-color:v2 and inspect the deployment rollout status.

    5. Check the rolling history of the deployment and on the student-node, save the current revision count number to the /opt/ocean-revision-count.txt file.

    6. Finally, perform a rollback and revert the deployment image to the older version.



    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    Use the following template to create a deployment called ocean-apd: -

    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: ocean-apd
      name: ocean-apd
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: ocean-apd
      strategy: 
       type: RollingUpdate
       rollingUpdate:
         maxUnavailable: 45%
         maxSurge: 55%
      template:
        metadata:
          labels:
            app: ocean-apd
        spec:
          containers:
          - image: kodekloud/webapp-color:v1
            name: webapp-color


    Now, create the deployment by using the kubectl create -f command in the default namespace: -

    kubectl create -f <FILE-NAME>.yaml


    After sometime, upgrade the deployment image to kodekloud/webapp-color:v2: -

    kubectl set image deploy ocean-apd webapp-color=kodekloud/webapp-color:v2


    And check out the rollout history of the deployment ocean-apd: -

    kubectl rollout history deploy ocean-apd
    deployment.apps/ocean-apd 
    REVISION  CHANGE-CAUSE
    1         <none>
    2         <none>


        NOTE: - Revision count is 2. In your lab, it could be different.


    On the student-node, store the revision count to the given file: -

    echo "2" > /opt/ocean-revision-count.txt


    In final task, rollback the deployment image to an old version: -

    kubectl rollout undo deployment ocean-apd


    Verify the image name by using the following command: -

    kubectl describe deploy ocean-apd | grep -i image


    It should be kodekloud/webapp-color:v1 image.

}


{

	student-node ~ ➜  kubectl config use-context cluster1
	Switched to context "cluster1".
	
	student-node ~ ➜  kubectl create namespace ckad15-memlt-ns-aecs
	namespace/ckad15-memlt-ns-aecs created
	
	student-node ~ ➜  cat << EOF | kubectl apply -f - -n ckad15-memlt-ns-aecs
	apiVersion: v1
	kind: LimitRange
	metadata:
	  name: ckad15-memlt-aecs
	spec:
	  limits:
	  - default:
	      memory: 512Mi
	    defaultRequest:
	      memory: 256Mi
	    type: Container
	EOF
	
	limitrange/ckad15-memlt-aecs created

}

{

	student-node ~ ➜  kubectl config use-context cluster2
	Switched to context "cluster2".
	
	student-node ~ ➜  k get po -n ckad13-ns-sa-aecs ckad13-nginx-pod-aecs -o yaml > ckad-pro-vol.yaml
	
	student-node ~ ➜  cat ckad-pro-vol.yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: ckad13-nginx-pod-aecs
	  namespace: ckad13-ns-sa-aecs
	spec:
	  containers:
	  - image: nginx
	    imagePullPolicy: Always
	    name: nginx
	.
	.
	.
	   volumeMounts:                              # Added
	    - mountPath: /var/run/secrets/tokens       # Added
	      name: vault-token                        # Added
	.
	.
	.
	  serviceAccount: ckad13-sa-aecs
	  serviceAccountName: ckad13-sa-aecs
	  volumes:
	  - name: vault-token                   # Added
	    projected:                          # Added
	      sources:                          # Added
	      - serviceAccountToken:            # Added
	          path: vault-token             # Added
	          expirationSeconds: 9000       # Added
	          audience: vault               # Added
	
	student-node ~ ➜  k replace -f ckad-pro-vol.yaml --force 
	pod "ckad13-nginx-pod-aecs" deleted
	pod/ckad13-nginx-pod-aecs replaced
}

{
	student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  kubectl get crd | grep -i anime
animes.animes.k8s.io

student-node ~ ➜  kubectl get crd animes.animes.k8s.io \
                 -o json \
                 | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties
{
  "animeName": {
    "type": "string"
  },
  "episodeCount": {
    "maximum": 52,
    "minimum": 24,
    "type": "integer"
  }
}

	student-node ~ ➜  k api-resources | grep anime
	animes                            an           animes.k8s.io/v1alpha1                 true         Anime
	
	student-node ~ ➜  cat << YAML | kubectl apply -f -
	 apiVersion: animes.k8s.io/v1alpha1
	 kind: Anime
	 metadata:
	   name: my-anime
	 spec:
	   animeName: "Death Note"
	   episodeCount: 37
	YAML
	anime.animes.k8s.io/my-anime created
	
	student-node ~ ➜  k get an my-anime 
	NAME       AGE
	my-anime   23s
}


{

	(	
		For this question, please set the context to cluster3 by running:

		kubectl config use-context cluster3		

		
		We have deployed some pods in the namespaces ckad-alpha and ckad-beta.
		
		You need to create a NetworkPolicy named ns-netpol-ckad that will restrict all Pods in Namespace ckad-alpha to only have outgoing traffic to Pods in Namespace ckad-beta . Ingress traffic should not be affected.
		
		However, the NetworkPolicy you create should allow egress traffic on port 53 TCP and UDP.
	)
	
	apiVersion: networking.k8s.io/v1
	kind: NetworkPolicy
	metadata:
	  name: ns-netpol-ckad
	  namespace: ckad-alpha
	spec:
	  podSelector: {}
	  policyTypes:
	  - Egress
	  egress:
	  - ports:
	    - port: 53
	      protocol: TCP
	    - port: 53
	      protocol: UDP
	  - to:
	     - namespaceSelector:
	        matchLabels:
	         kubernetes.io/metadata.name: ckad-beta
	
}


{
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have deployed several applications in the ns-ckad17-svcn namespace that are exposed inside the cluster via ClusterIP.


    Your task is to create a LoadBalancer type service that will serve traffic to the applications based on its labels. Create the resources as follows:
    Service lb1-ckad17-svcn for serving traffic at port 31890 to pods with labels "exam=ckad, criteria=location".

    Service lb2-ckad17-svcn for serving traffic at port 31891 to pods with labels "exam=ckad, criteria=cpu-high".

    info_outline
    Solution

    To create the loadbalancer for the pods with the specified lables, first we need to find the pods with the mentioned lables.
    To get pods with labels "exam=ckad, criteria=location"

    kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=location
    -----
    NAME               READY   STATUS    RESTARTS   AGE
    geo-location-app   1/1     Running   0          10m

    Similarly to get pods with labels "exam=ckad,criteria=cpu-high".

    kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=cpu-high
    -----
    NAME           READY   STATUS    RESTARTS   AGE
    cpu-load-app   1/1     Running   0          11m

    Now we know which pods use the labels, we can create the LoadBalancer type service using the imperative command.

    kubectl -n ns-ckad17-svcn expose pod geo-location-app --type=LoadBalancer --name=lb1-ckad17-svcn

    Similarly, create the another service.

    kubectl -n ns-ckad17-svcn expose pod cpu-load-app --type=LoadBalancer --name=lb2-ckad17-svcn


    Once the services are created, you can edit the services to use the correct nodePorts as per the question using kubectl -n ns-ckad17-svcn edit svc lb2-ckad17-svcn. 
}

{
	apiVersion: v1
kind: Service
metadata:
  name: ckad14-svcn
spec:
  type: ExternalName
  externalName: 10.0.0.3
  ports:
    - name: http
      port: 80
      targetPort: 80
}

{
	To create the loadbalancer for the pods with the specified lables, first we need to find the pods with the mentioned lables.
	To get pods with labels "exam=ckad, criteria=location"
	
	kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=location
	-----
	NAME               READY   STATUS    RESTARTS   AGE
	geo-location-app   1/1     Running   0          10m
	
	Similarly to get pods with labels "exam=ckad,criteria=cpu-high".
	
	kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=cpu-high
	-----
	NAME           READY   STATUS    RESTARTS   AGE
	cpu-load-app   1/1     Running   0          11m
	
	Now we know which pods use the labels, we can create the LoadBalancer type service using the imperative command.
	
	kubectl -n ns-ckad17-svcn expose pod geo-location-app --type=LoadBalancer --name=lb1-ckad17-svcn
	
	Similarly, create the another service.
	
	kubectl -n ns-ckad17-svcn expose pod cpu-load-app --type=LoadBalancer --name=lb2-ckad17-svcn
	
	
	Once the services are created, you can edit the services to use the correct nodePorts as per the question using kubectl -n ns-ckad17-svcn edit svc lb2-ckad17-svcn. 
}

{
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster2 by running:

    kubectl config use-context cluster2


    Create a role configmap-updater in the ckad21-auth2-aecs namespace granting the update and get permissions on configmaps resources but restricted to only the ckad-cnfmp-aecs instance of the resource.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster2
    Switched to context "cluster2".

    student-node ~ ➜  k get cm -n ckad21-auth2-aecs 
    NAME               DATA   AGE
    kube-root-ca.crt   1      3m35s
    ckad-cnfmp-aecs    2      3m35s

    student-node ~ ➜  kubectl create role configmap-updater --namespace=ckad21-auth2-aecs --resource=configmaps --resource-name=ckad-cnfmp-aecs --verb=update,get 
    role.rbac.authorization.k8s.io/configmap-updater created

    student-node ~ ➜  k get role -n ckad21-auth2-aecs configmap-updater -o yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      creationTimestamp: "2023-03-22T09:01:04Z"
      name: configmap-updater
      namespace: ckad21-auth2-aecs
      resourceVersion: "2799"
      uid: c152750a-198e-438e-9993-64b3e872c3e0
    rules:
    - apiGroups:
      - ""
      resourceNames:
      - ckad-cnfmp-aecs
      resources:
      - configmaps
      verbs:
      - update
      - get


}

# 14, 


{

	student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  k get pods -n ckad05-securityctx-aecs 
NAME                    READY   STATUS    RESTARTS   AGE
ckad05-multi-pod-aecs   2/2     Running   0          2m47s

student-node ~ ➜  k exec -i -n ckad05-securityctx-aecs ckad05-multi-pod-aecs -c web -- id
uid=1001 gid=0(root) groups=0(root)

student-node ~ ➜  k get pods -n ckad05-securityctx-aecs ckad05-multi-pod-aecs -o yaml > security-ctx-web.yaml

student-node ~ ➜  vim security-ctx-web.yaml

student-node ~ ➜  cat security-ctx-web.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: ckad05-multi-pod-aecs
  namespace: ckad05-securityctx-aecs
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     securityContext:
      runAsUser: 1002
     command: ["sleep", "5000"]
  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]

student-node ~ ➜ k replace -f security-ctx-web.yaml --force 
pod "ckad05-multi-pod-aecs" deleted
pod/ckad05-multi-pod-aecs replaced

student-node ~ ➜  k exec -i -n ckad05-securityctx-aecs ckad05-multi-pod-aecs -c web -- id
uid=1002 gid=0(root) groups=0(root)

student-node ~ ➜  k exec -i -n ckad05-securityctx-aecs ckad05-multi-pod-aecs -c sidecar -- id
uid=1001 gid=0(root) groups=0(root)
}

{
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have deployed an application in the green-space namespace. we also deployed the ingress controller and the ingress resource.


    However, currently, the ingress controller is not working as expected. Inspect the ingress definitions and troubleshoot the issue so that the services are accessible as per the ingress resource definition.

    Also, update the path for the app-wear-service to /app-wear and app-video-service to /app-video.



    Note: You are allowed to edit or delete the resources related to ingress but do not change the pods.

    info_outline
    Solution

    Check the status of the ingress, pods, and application related services.

    cluster3-controlplane ~ ➜  k get pods -n ingress-nginx 
    NAME                                        READY   STATUS      RESTARTS      AGE
    ingress-nginx-admission-create-l6fgw        0/1     Completed   0             11m
    ingress-nginx-admission-patch-sfgc4         0/1     Completed   0             11m
    ingress-nginx-controller-5f8964959d-278rc   0/1     Error       2 (26s ago)   29s

    You would see an Error or CrashLoopBackOff in the ingress-nginx-controller. Inspect the logs of the controller pod.

    cluster3-controlplane ~ ✖ k logs -n ingress-nginx ingress-nginx-controller-5f8964959d-278rc 
    -------------------------------------------------------------------------------
    --------
    F0316 08:03:28.111614      57 main.go:83] No service with name default-backend-service found in namespace default:

    -------

    You see an error msg saying "No service with name default-backend-service found in namespace default".

    We don't have the service with that name in the default namespace, so we need to edit the ingress controller deployment to use the service that we have .i.e. default-backend-service in the green-space namespace.

    To create the controller deployment with correct backend service, first save the deployment in a file, delete the controller deployment, edit the file and create the deployment.

        Save the deployment in file

    k get -n ingress-nginx deployments.apps ingress-nginx-controller -o yaml >> ing-control.yaml

        Delete the deployment.
        k delete -n ingress-nginx deploy ingress-nginx-controller
        Edit the file to match the correct service.

         spec:
            containers:
            - args:
              - /nginx-ingress-controller
              - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
              - --election-id=ingress-controller-leader
              - --watch-ingress-without-class=true
              - --default-backend-service=green-space/default-backend-service   #Changed to correct namespace
              - --controller-class=k8s.io/ingress-nginx
              - --ingress-class=nginx
              - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
              - --validating-webhook=:8443
              - --validating-webhook-certificate=/usr/local/certificates/cert
              - --validating-webhook-key=/usr/local/certificates/key

        Apply the manifest, it should be up and running.

}

Random Notes for k8s

# this could be used to get the logs from a cronjob maybe
kubectl exec -n ckad-multi-containers ckad-sidecar-pod --container main-container -- cat /usr/share/nginx/html/index.html


# Ultimate sleeper command
command: ["sh", "-c", "sleep 5000;"]


# X: {
	SECTION: APPLICATION DESIGN AND BUILD
	
	
	For this question, please set the context to cluster1 by running:
	
	kubectl config use-context cluster1
	
	
	
	In the ckad-multi-containers namespaces, create a ckad-sidecar-pod pod that matches the following requirements.
	
	
	Pod has an emptyDir volume named my-vol.
	
	
	The first container named main-container, runs nginx:1.16 image. This container mounts the my-vol volume at /usr/share/nginx/html path.
	
	
	The second container named sidecar-container, runs busybox:1.28 image. This container mounts the my-vol volume at /var/log path.
	
	Every 5 seconds, this container should write the current date along with greeting message Hi I am from Sidecar container to index.html in the my-vol volume.
}



answer: {
	apiVersion: v1
	kind: Pod
	metadata:
	  namespace: ckad-multi-containers
	  name: ckad-sidecar-pod
	spec:
	  containers:
	    - image: nginx:1.16
	      name: main-container
	      resources: {}
	      ports:
	        - containerPort: 80
	      volumeMounts:
	        - name: my-vol
	          mountPath: /usr/share/nginx/html
	    - image: busybox:1.28
	      command:
	        - /bin/sh
	        - -c
	        - while true; do echo $(date -u) Hi I am from Sidecar container >> /var/log/index.html; sleep 5;done
	      name: sidecar-container
	      resources: {}
	      volumeMounts:
	        - name: my-vol
	          mountPath: /var/log
	  dnsPolicy: Default

	  # Location to write to
	  volumes:
	    - name: my-vol
	      emptyDir: {}
}















helm thingy: {
	
    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


	(
    	Our new client wants to deploy the resources through the popular Helm tool. 
		In the initial phase, our team lead wants to deploy nginx, 
			a very powerful and versatile web server software that is widely used to serve static content, reverse proxying, load balancing, 
			from the bitnami helm chart on the cluster3-controlplane node.
	)

    The chart URL and other specifications are as follows: -

    1. The chart URL link - https://charts.bitnami.com/bitnami

    2. The chart repository name should be polar.

    3. The release name should be nginx-server.

    4. All the resources should be deployed on the cd-tool-apd namespace.

	(
		Is Helm repository created?
		Is Helm chart installed?
		Are resources deployed on ns?
	)

	

    NOTE: - You have to perform this task from the student-node.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the helm commands. Here are the steps: -

    Add the repostiory to Helm with the following command: -

    helm repo add polar https://charts.bitnami.com/bitnami


    The helm repo add command is used to add a new chart repository to Helm and this allows us to browse and install charts from the new repository using the Helm package manager.


    Use the helm repo ls command which is used to list all the currently configured Helm chart repositories on Kubernetes cluster.

    helm repo ls 


    Search for the nginx chart in a polar chart repository as follows: -

    helm search repo polar | grep nginx



    The helm search repo command is used to search for charts in a specified Helm chart repository. Also, it allows us to browse available charts and view their details, such as their name, version, description, and maintainers.

    Before installing the chart, we have to create a namespace as given in the task description. Then we can install the nginx chart on a Kubernetes cluster.

    kubectl create ns cd-tool-apd

    helm install nginx-server polar/nginx -n cd-tool-apd


	v1
	{

		{
			- What needs to happen:
				1) add the repo to helm
				2) install the chart (with a namespace)
		}


		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm install nginx-server polar/nginx -n cd-tool-apd
		}

	};

	v2 (This is specifically for the question)
	{

		{
			- to do:
				1) add the repo to helm
				2) search for the chart you need
				3) create the namespace that is needed
				4) install the chart with the namespace
		}

		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm search repo polar | grep nginx
				3) k create ns cd-tool-apd
				4) helm install nginx-server polar/nginx -n cd-tool-apd
		}
		
	}

}


{

	*kubectl rollout undo -n dev-apd deploy webapp-apd
	k rollout undo -n dev-apd deployment/webapp-apd

	k rollout history deploy [deployment-name-goes-here]

	echo "2" > /opt/ocean-revision-count.txt

	k describe deployment ocean-apd | grep -i image

	kubectl top nodes > /root/node-metrics

	kubectl logs kodekloud-logs-aom -c sidecar > /root/ckad-exam.aom

}





























{
	WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config
NAME (release-name)     NAMESPACE               REVISION        UPDATED                                     STATUS          CHART                           APP VERSION
atlanta-page-apd        atlanta-page-04         1               2023-07-12 10:18:51.122388204 +0000 UTC     deployed        atlanta-page-apd-0.1.0          1.16.0     
digi-locker-apd         digi-locker-02          1               2023-07-12 10:18:48.244000366 +0000 UTC     deployed        digi-locker-apd-0.1.0           1.16.0     
security-alpha-apd      security-alpha-01       1               2023-07-12 10:18:46.773087487 +0000 UTC     deployed        security-alpha-apd-0.1.0        1.16.0     
traefik                 kube-system             1               2023-07-12 10:00:51.717285983 +0000 UTC     deployed        traefik-20.3.1+up20.3.0         v2.9.4     
traefik-crd             kube-system             1               2023-07-12 10:00:31.013404459 +0000 UTC     deployed        traefik-crd-20.3.1+up20.3.0     v2.9.4     
web-dashboard-apd       web-dashboard-03        1               2023-07-12 10:18:49.727029014 +0000 UTC     deployed        web-dashboard-apd-0.1.0         1.16.0     


kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n atlanta-page-04 atlanta-page-apd -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n digi-locker-02 digi-locker-apd  -o json | jq -r '.spec.template.spec.containers[].image'




kubectl get deploy -n security-alpha-01 security-alpha-apd  -o json | jq -r '.spec.template.spec.containers[].image' 



    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    On the cluster1, the team has installed multiple helm charts on a different namespace. By mistake, those deployed resources include one of the vulnerable images called kodekloud/click-counter:latest. Find out the release name and uninstall it.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster1


    In this task, we will use the helm commands and jq tool. Here are the steps: -


        Run the helm ls command with -A option to list the releases deployed on all the namespaces using helm.

    helm ls -A


        We will use the jq tool to extract the image name from the deployments.

    kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


    Replace <NAMESPACE> with the namespace and <DEPLOYMENT-NAME> with the deployment name, which we get from the previous commands.

    After finding the kodekloud/click-counter:latest image, use the helm uninstall to remove the deployed chart that are using this vulnerable image.

    helm uninstall <RELEASE-NAME> -n <NAMESPACE> 
}



{ : Service Account :

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl command. Here are the steps: -


    Use the kubectl get command to list all the given resources: -

kubectl get po,deploy,sa,ns -n fusion-apd-x1df5


Here -n option stands for namespace, which is used to specify the namespace.

The above command will list all the resources from the fusion-apd-x1df5 namespace.

    Inspect the service account is used by the pods/deployment.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -oyaml



The deployment is using the default service account.


    Now, use the kubectl get command to retrieves the YAML definition of a deployment named robox-west-apd and save it into a file.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -o yaml > <FILE-NAME>.yaml


    Open a VI editor. Make the necessary changes and save it. It should look like this: -

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    global-kgh: robox-west-apd
  name: robox-west-apd
  namespace: fusion-apd-x1df5
spec:
  replicas: 3
  selector:
    matchLabels:
      global-kgh: robox-west-apd
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        global-kgh: robox-west-apd
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: robox-container
      serviceAccountName: galaxy-apd-xb12 ( <--------- )


    Now, replace the resource with the following command:

kubectl replace -f <FILE-NAME>.yaml --force


The above command will delete the existing deployment and create a new one with changes in the given namespace.



}



{ : Broken :


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


    After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.

    NOTE: - We have to perform this task on the cluster3-controlplane node.


    You can SSH into the cluster3 using ssh cluster3-controlplane command.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the kubectl and helm commands. Here are the steps: -


        Log in to the cluster3-controlplane node first and use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

    helm ls -A


    Here -A or --all-namespaces option lists all the releases of all the namespaces.


        Identify the namespace where the resources get deployed.


        Use the helm repo ls command to list the helm repositories.

    helm repo ls 


        Now, update the helm repository with the following command: -

    helm repo update lvm-crystal-apd -n crystal-apd-ns


    The above command updates the local cache of available charts from the configured chart repositories.


        The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.

    helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30



    The -l or --versions option is used to display information about all available chart versions.


        Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

    helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2


        After upgrading the chart version, you can verify it with the following command: -

    helm ls -n crystal-apd-ns


    Look under the CHART column for the chart version.


        Use the kubectl get command to check the replicas of the deployment: -

    kubectl get deploy -n crystal-apd-ns


    The available count 2 is under the AVAILABLE column.



	
}



{ : Update with Kubectl : 
		(
			[kubectl] [set] [image] [-n] [type the namespace here] [deploy(short for deployments.apps)] [deployment name (use k get {-n some-namespace-goes-here} deployment.apps) (and maybe add the namespace)] [container name goes here]=[container-image-version-with-a-tag (: is the tag with fun decimal numbers after it)]
		)

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl describe, kubectl get, kubectl set and kubectl scale commands. Here are the steps: -

    To check all the deployments in all the namespaces in the cluster2, we would have to run the following command:

kubectl get deployments -A


Inspect all the deployments.


    We can see that one of the deployment's names is results-apd and deployed on dashboard-apd namespace. Use the kubectl describe command to get detailed information of that deployment: -

kubectl describe -n dashboard-apd deploy results-apd


The output of the kubectl describe command will provide you with a detailed description of the deployment, including its name, namespace, creation time, labels, replicas, and the Docker image being used.


    In the previous command, we can see the container and image name under the Pod Template spec. Use the kubectl set command to update the image of that container as follows:

kubectl set image -n dashboard-apd deploy results-apd results-apd-container=nginx:1.23.3


After running the above command, Kubernetes will automatically update the results-apd-container container with the new image, and create a new replica of the resource with the updated image.
The old replica will be deleted once the new one is up and running.


    Now, SSH to the cluster2-controlplane node and use echo command to add this new image to a file at /root/records/new-image-records.txt: -

echo "nginx:1.23.3" > /root/records/new-image-records.txt


If the records directory is absent, use the mkdir command to create this directory.


    NOTE: - To exit from any node, type exit on the terminal or press CTRL + D.


    Now, run the kubectl scale command to scale the deployment to 4:

kubectl scale deployment -n dashboard-apd results-apd --replicas=4

[k] [scale] [deployment] [-n] ["some-namespace"] [deployment name] [--replicas]=[any number 0-9?]


Cross-verify the scaled deployment by using the kubectl get command:

kubectl get deployments,pods -n dashboard-apd



}




{

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl and helm commands. Here are the steps: -


    To check all the resources in all namespaces in the cluster2, we would have to run the following command:

kubectl get all -A


It will list all the available resources of all namespaces.


We can see that the resources have used prefix called garuda in the name.

    To list all of the releases on the garuda-apps-apd namespace. Run the following command as follows: -

helm ls -n garuda-apps-apd


We can see the release names of the web and security applications, and they are deployed on the garuda-apps-apd namespace.


    Write their release names by using the echo command as follows: -

echo "garuda-secret-apd,garuda-web-apd" > /root/apps-release-names.txt


    NOTE: - Make sure you have logged in student-node.


    On the same cluster, one more testing application is running on the testing-apd namespace, which we must delete because it is consuming resources.

helm ls -n testing-apd


helm uninstall -n testing-apd image-scanner 


}














{
	student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs 
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          7m57s

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs -o yaml  > 1-pod-cm.yaml

# Using editor to modify the fields as per the requirements:
student-node ~ ➜  vim 1-pod-cm.yaml 

student-node ~ ➜  cat 1-pod-cm.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: ckad02-test-pod-aecs
  namespace: ckad02-mult-cm-cfg-aecs
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: ["/bin/sh", "-c", "while true; do env | egrep \"GREETINGS|WHO\"; sleep 10; done"]
      env:
        - name: GREETINGS
          valueFrom:
            configMapKeyRef:
              name: ckad02-config1-aecs
              key: greetings.how
        - name: WHO
          valueFrom:
            configMapKeyRef:
              name: ckad02-config2-aecs
              key: man
  restartPolicy: Always

student-node ~ ➜  k replace -f 1-pod-cm.yaml --force
pod "ckad02-test-pod-aecs" deleted
pod/ckad02-test-pod-aecs replaced

student-node ~ ➜  k get -f 1-pod-cm.yaml
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          10s

student-node ~ ➜  k logs -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs | head -2
GREETINGS=HI
WHO=HANDSOME
}




{
	kubectl rollout history deployment -n blue-apd foundary-apd --revision=3
}
32043


kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt











{

 Run the following command to change the context: -

kubectl config use-context cluster1


In this task, we will use the helm commands. Here are the steps: -


    Use the helm ls command to list the release deployed on the default namespace using helm.

helm ls -n default


    First, validate the helm chart by using the helm lint command: -

cd /root/

helm lint ./new-version


    Now, install the new version of the application by using the helm install command as follows: -

helm install --generate-name ./new-version


We haven't got any release name in the task, so we can generate the random name from the --generate-name option.

Finally, uninstall the old version of the application by using the helm uninstall command: -

helm uninstall webpage-server-01 -n default

	
}



{


	student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  k get po -n ckad13-ns-sa-aecs ckad13-nginx-pod-aecs -o yaml > ckad-pro-vol.yaml

student-node ~ ➜  cat ckad-pro-vol.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ckad13-nginx-pod-aecs
  namespace: ckad13-ns-sa-aecs
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
.
.
.
   volumeMounts:                              # Added
    - mountPath: /var/run/secrets/tokens       # Added
      name: vault-token                        # Added
.
.
.
  serviceAccount: ckad13-sa-aecs
  serviceAccountName: ckad13-sa-aecs
  volumes:
  - name: vault-token                   # Added
    projected:                          # Added
      sources:                          # Added
      - serviceAccountToken:            # Added
          path: vault-token             # Added
          expirationSeconds: 9000       # Added
          audience: vault               # Added

student-node ~ ➜  k replace -f ckad-pro-vol.yaml --force 
pod "ckad13-nginx-pod-aecs" deleted
pod/ckad13-nginx-pod-aecs replaced


}



{

    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    Create a service account named ckad23-sa-aecs in the namespace ckad23-nssa-aecs.

    Grant the service account get and list permissions to access all resources within the namespace using a 
		Role named wide-access-aecs.

    Also bind the Role to the service account using a RoleBinding named wide-access-rb-aecs, restricting 
		the access to the ckad23-nssa-aecs namespace only.


    Note: If the resources do not exist, please create them as well.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create ns ckad23-nssa-aecs
    namespace/ckad23-nssa-aecs created

    student-node ~ ➜  kubectl create serviceaccount ckad23-sa-aecs -n ckad23-nssa-aecs
    serviceaccount/ckad23-sa-aecs created

    student-node ~ ➜  kubectl create role wide-access-aecs --namespace=ckad23-nssa-aecs 
		--verb=get,list --resource=* 
    		role.rbac.authorization.k8s.io/wide-access-aecs created

    student-node ~ ➜  kubectl create rolebinding wide-access-rb-aecs \
       --role=wide-access-aecs \
       --serviceaccount=ckad23-nssa-aecs:ckad23-sa-aecs \
       --namespace=ckad23-nssa-aecs
    rolebinding.rbac.authorization.k8s.io/wide-access-rb-aecs created

	(
		1) create the Namespace

		2) create the Service Account

		3) create the role

		4) create the RoleBinding
	)

	(
		Is service account created?
		Are roles created correctly?
		Is correct role defined for rolebinding?
		Is correct Service account specified for rolebinding
	)

	
}


{
	# create service account
kubectl create serviceaccount pod-viewer

# Create cluster role/role
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-viewer
rules:
- apiGroups: [""] # core API group
  resources: ["pods", "namespaces"]
  verbs: ["get", "watch", "list"]
---

# create cluster role binding
kubectl create clusterrolebinding pod-viewer \
  --clusterrole=pod-viewer \
  --serviceaccount=default:pod-viewer

# get service account secret
kubectl get secret | grep pod-viewer
pod-viewer-token-6fdcn   kubernetes.io/service-account-token   3      2m58s

# get token
kubectl describe secret pod-viewer-token-6fdcn
Name:         pod-viewer-token-6fdcn
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: pod-viewer
              kubernetes.io/service-account.uid: bbfb3c4e-2254-11ea-a26c-0242ac110009

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InBvZC12aWV3ZXItdG9rZW4tNmZkY24iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicG9kLXZpZXdlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImJiZmIzYzRlLTIyNTQtMTFlYS1hMjZjLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnBvZC12aWV3ZXIifQ.Pgco_4UwTCiOfYYS4QLwqgWnG8nry6JxoGiJCDuO4ZVDWUOkGJ3w6-8K1gGRSzWFOSB8E0l2YSQR4PB9jlc_9GYCFQ0-XNgkuiZBPvsTmKXdDvCNFz7bmg_Cua7HnACkKDbISKKyK4HMH-ShgVXDoMG5KmQQ_TCWs2E_a88COGMA543QL_BxckFowQZk19Iq8yEgSEfI9m8qfz4n6G7dQu9IpUSmVNUVB5GaEsaCIg6h_AXxDds5Ot6ngWUawvhYrPRv79zVKfAxYKwetjC291-qiIM92XZ63-YJJ3xbxPAsnCEwL_hG3P95-CNzoxJHKEfs_qa7a4hfe0k6HtHTWA
ca.crt:     1025 bytes
namespace:  7 bytes
```

Login to dashboard using the above token. you should see only pods and namespaces

[![Refer the below link][1]][1]


  [1]: https://i.stack.imgur.com/D9bDi.png
}

13, 14, 15, 16, 17, 18, 20


{ 13
    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster2 by running:

    kubectl config use-context cluster2


    Create a custom resource my-anime of kind Anime with the below specifications:

    Name of Anime: Death Note
    Episode Count: 37


    TIP: You may find the respective CRD with anime substring in it.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster2
    Switched to context "cluster2".

    student-node ~ ➜  kubectl get crd | grep -i anime
    animes.animes.k8s.io

    student-node ~ ➜  kubectl get crd animes.animes.k8s.io \
                     -o json \
                     | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties
    {
      "animeName": {
        "type": "string"
      },
      "episodeCount": {
        "maximum": 52,
        "minimum": 24,
        "type": "integer"
      }
    }

    student-node ~ ➜  k api-resources | grep anime
    animes                            an           animes.k8s.io/v1alpha1                 true         Anime

    student-node ~ ➜  cat << YAML | kubectl apply -f -
     apiVersion: animes.k8s.io/v1alpha1
     kind: Anime
     metadata:
       name: my-anime
     spec:
       animeName: "Death Note"
       episodeCount: 37
    YAML
    anime.animes.k8s.io/my-anime created

    student-node ~ ➜  k get an my-anime 
    NAME       AGE
    my-anime   23s


	
}


{ 14
    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Update pod ckad06-cap-aecs in the namespace ckad05-securityctx-aecs to run as root user and with the SYS_TIME and NET_ADMIN capabilities.


    Note: Make only the necessary changes. Do not modify the name of the pod.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
        terminationMessagePath: /dev/termination-log

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml > pod-capabilities.yaml

    student-node ~ ➜  vim pod-capabilities.yaml

    student-node ~ ➜  cat pod-capabilities.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad06-cap-aecs
      namespace: ckad05-securityctx-aecs
    spec:
      containers:
      - command:
        - sleep
        - "4800"
        image: ubuntu
        name: ubuntu-sleeper
        securityContext:
          capabilities:
            add: ["SYS_TIME", "NET_ADMIN"]

    student-node ~ ➜  k replace -f pod-capabilities.yaml --force 
    pod "ckad06-cap-aecs" deleted
    pod/ckad06-cap-aecs replaced

    student-node ~ ➜  k get -n ckad05-securityctx-aecs pod ckad06-cap-aecs -o yaml | egrep -i -A3 capabilities:
          capabilities:
            add:
            - SYS_TIME
            - NET_ADMIN

}

{ 15
	    kubectl config use-context cluster1


    Create a pod named ckad17-qos-aecs-3 in namespace ckad17-nqoss-aecs with image nginx and container name ckad17-qos-ctr-3-aecs.

    Define other fields such that the Pod is configured to use the Quality of Service (QoS) class of Burstable.


    Also retrieve the name and QoS class of each Pod in the namespace ckad17-nqoss-aecs in the below format and save the output to a file named qos_status_aecs in the /root directory.

    Format:

    NAME    QOS
    pod-1   qos_class
    pod-2   qos_class

    info_outline
    Solution

    student-node ~ ➜ kubectl config use-context cluster1
    Switched to context "cluster1".

    student-node ~ ➜  cat << EOF | kubectl apply -f -
    apiVersion: v1
    kind: Pod
    metadata:
      name: ckad17-qos-aecs-3
      namespace: ckad17-nqoss-aecs
    spec:
      containers:
      - name: ckad17-qos-ctr-3-aecs
        image: nginx
        resources:
          limits:
            memory: "200Mi"
          requests:
            memory: "100Mi"
    EOF

    pod/ckad17-qos-aecs-3 created

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
    NAME                QOS
    ckad17-qos-aecs-1   BestEffort
    ckad17-qos-aecs-2   Guaranteed
    ckad17-qos-aecs-3   Burstable

    student-node ~ ➜  kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs


}


{ 16
	    SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    Create a ClusterRole named healthz-access that allows GET and POST requests to the non-resource endpoint /healthz and all subpaths.


    Bound this ClusterRole to a user healthz-user using a ClusterRoleBinding named healthz-access-binding.

    info_outline
    Solution

    student-node ~ ➜  kubectl config use-context cluster3
    Switched to context "cluster3".

    student-node ~ ➜  kubectl create clusterrole healthz-access --non-resource-url=/healthz,/healthz/* --verb=get,post
    clusterrole.rbac.authorization.k8s.io/healthz-access created


    student-node ~ ➜  kubectl create clusterrolebinding healthz-access-binding --clusterrole=healthz-access --user=healthz-user
    clusterrolebinding.rbac.authorization.k8s.io/healthz-access-binding created


}


{ 17

	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    You are requested to create a network policy named deny-all-svcn that denies all incoming and outgoing traffic to ckad12-svcn namespace.


    Note: The namespace ckad12-svcn doesn't exist. Create the namespace before creating the Policy.

    info_outline
    Solution

    Create the namespace using the following command - kubectl create ns ckad12-svcn

    Create the network policy in the namespace ckad12-svcn using the following manifest.

    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: deny-all-svcn
      namespace: ckad12-svcn
    spec:
      podSelector: {}
      policyTypes:
      - Ingress
      - Egress


	
}

{ 18
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3



    We have deployed a pod pod22-ckad-svcn in the default namespace. Create a service svc22-ckad-svcn that will expose the pod at port 6335.

    Note: Use the imperative command for the above scenario.

    info_outline
    Solution

    To use the cluster 3, switch the context using:

    kubectl config use-context cluster3

    To expose the pod pod22-ckad-svcn at port 6335, we can use the following imperative command.

    kubectl expose pod pod22-ckad-svcn --name=svc22-ckad-svcn --port=6335

    It will create a service with name svc22-ckad-svcn and pod will be exposed at port 6335.
}


{ 20
	    SECTION: SERVICES AND NETWORKING



    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    Create an nginx pod called nginx-resolver-ckad03-svcn using image nginx, and expose it internally at port 80 with a service called nginx-resolver-service-ckad03-svcn.


    info_outline
    Solution

    Switching to cluster1:


    kubectl config use-context cluster1



    To create a pod nginx-resolver-ckad03-svcn and expose it internally:


    student-node ~ ➜ kubectl run nginx-resolver-ckad03-svcn --image=nginx 
    student-node ~ ➜ kubectl expose pod/nginx-resolver-ckad03-svcn --name=nginx-resolver-service-ckad03-svcn --port=80 --target-port=80 --type=ClusterIP 


}


{

	Run the following command to change the context: -

	kubectl config use-context cluster1
	
	
	In this task, we will use the kubectl command. Here are the steps: -
	
	The cube-alpha-apd and ruby-alpha-apd deployment has 5-5 replicas. The alpha-apd-service service now routes traffic to 10 pods in total (5 replicas on the ruby-alpha-apd deployment and 5 replicas from cube-alpha-apd deployment).
	
	Use the kubectl get command to list the following deployments: -
	
	kubectl get deploy -n alpha-ns-apd
	
	
	
	Since the service distributes traffic to all pods equally, in this case, approximately 50% of the traffic will go to cube-alpha-apd deployment.
	
	To reduce this below 40%, scale down the pods on the cube-alpha-apd deployment to the minimum to 2.
	
	kubectl scale deployment --replicas=2 cube-alpha-apd -n alpha-ns-apd
	
	
	Once this is done, only ~40% of traffic sho

}









































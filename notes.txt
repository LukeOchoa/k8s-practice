Random Notes for k8s

# this could be used to get the logs from a cronjob maybe
kubectl exec -n ckad-multi-containers ckad-sidecar-pod --container main-container -- cat /usr/share/nginx/html/index.html


# Ultimate sleeper command
command: ["sh", "-c", "sleep 5000;"]


# X: {
	SECTION: APPLICATION DESIGN AND BUILD
	
	
	For this question, please set the context to cluster1 by running:
	
	kubectl config use-context cluster1
	
	
	
	In the ckad-multi-containers namespaces, create a ckad-sidecar-pod pod that matches the following requirements.
	
	
	Pod has an emptyDir volume named my-vol.
	
	
	The first container named main-container, runs nginx:1.16 image. This container mounts the my-vol volume at /usr/share/nginx/html path.
	
	
	The second container named sidecar-container, runs busybox:1.28 image. This container mounts the my-vol volume at /var/log path.
	
	Every 5 seconds, this container should write the current date along with greeting message Hi I am from Sidecar container to index.html in the my-vol volume.
}



answer: {
	apiVersion: v1
	kind: Pod
	metadata:
	  namespace: ckad-multi-containers
	  name: ckad-sidecar-pod
	spec:
	  containers:
	    - image: nginx:1.16
	      name: main-container
	      resources: {}
	      ports:
	        - containerPort: 80
	      volumeMounts:
	        - name: my-vol
	          mountPath: /usr/share/nginx/html
	    - image: busybox:1.28
	      command:
	        - /bin/sh
	        - -c
	        - while true; do echo $(date -u) Hi I am from Sidecar container >> /var/log/index.html; sleep 5;done
	      name: sidecar-container
	      resources: {}
	      volumeMounts:
	        - name: my-vol
	          mountPath: /var/log
	  dnsPolicy: Default

	  # Location to write to
	  volumes:
	    - name: my-vol
	      emptyDir: {}
}















helm thingy: {
	
    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


	(
    	Our new client wants to deploy the resources through the popular Helm tool. 
		In the initial phase, our team lead wants to deploy nginx, 
			a very powerful and versatile web server software that is widely used to serve static content, reverse proxying, load balancing, 
			from the bitnami helm chart on the cluster3-controlplane node.
	)

    The chart URL and other specifications are as follows: -

    1. The chart URL link - https://charts.bitnami.com/bitnami

    2. The chart repository name should be polar.

    3. The release name should be nginx-server.

    4. All the resources should be deployed on the cd-tool-apd namespace.

	(
		Is Helm repository created?
		Is Helm chart installed?
		Are resources deployed on ns?
	)

	

    NOTE: - You have to perform this task from the student-node.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the helm commands. Here are the steps: -

    Add the repostiory to Helm with the following command: -

    helm repo add polar https://charts.bitnami.com/bitnami


    The helm repo add command is used to add a new chart repository to Helm and this allows us to browse and install charts from the new repository using the Helm package manager.


    Use the helm repo ls command which is used to list all the currently configured Helm chart repositories on Kubernetes cluster.

    helm repo ls 


    Search for the nginx chart in a polar chart repository as follows: -

    helm search repo polar | grep nginx



    The helm search repo command is used to search for charts in a specified Helm chart repository. Also, it allows us to browse available charts and view their details, such as their name, version, description, and maintainers.

    Before installing the chart, we have to create a namespace as given in the task description. Then we can install the nginx chart on a Kubernetes cluster.

    kubectl create ns cd-tool-apd

    helm install nginx-server polar/nginx -n cd-tool-apd


	v1
	{

		{
			- What needs to happen:
				1) add the repo to helm
				2) install the chart (with a namespace)
		}


		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm install nginx-server polar/nginx -n cd-tool-apd
		}

	};

	v2 (This is specifically for the question)
	{

		{
			- to do:
				1) add the repo to helm
				2) search for the chart you need
				3) create the namespace that is needed
				4) install the chart with the namespace
		}

		{
			- List of actions:
				1) helm repo add polar https://charts.bitnami.com/bitnami
				2) helm search repo polar | grep nginx
				3) k create ns cd-tool-apd
				4) helm install nginx-server polar/nginx -n cd-tool-apd
		}
		
	}

}


{

	*kubectl rollout undo -n dev-apd deploy webapp-apd
	k rollout undo -n dev-apd deployment/webapp-apd

	k rollout history deploy [deployment-name-goes-here]

	echo "2" > /opt/ocean-revision-count.txt

	k describe deployment ocean-apd | grep -i image

	kubectl top nodes > /root/node-metrics

	kubectl logs kodekloud-logs-aom -c sidecar > /root/ckad-exam.aom

}





























{
	WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /root/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /root/.kube/config
NAME (release-name)     NAMESPACE               REVISION        UPDATED                                     STATUS          CHART                           APP VERSION
atlanta-page-apd        atlanta-page-04         1               2023-07-12 10:18:51.122388204 +0000 UTC     deployed        atlanta-page-apd-0.1.0          1.16.0     
digi-locker-apd         digi-locker-02          1               2023-07-12 10:18:48.244000366 +0000 UTC     deployed        digi-locker-apd-0.1.0           1.16.0     
security-alpha-apd      security-alpha-01       1               2023-07-12 10:18:46.773087487 +0000 UTC     deployed        security-alpha-apd-0.1.0        1.16.0     
traefik                 kube-system             1               2023-07-12 10:00:51.717285983 +0000 UTC     deployed        traefik-20.3.1+up20.3.0         v2.9.4     
traefik-crd             kube-system             1               2023-07-12 10:00:31.013404459 +0000 UTC     deployed        traefik-crd-20.3.1+up20.3.0     v2.9.4     
web-dashboard-apd       web-dashboard-03        1               2023-07-12 10:18:49.727029014 +0000 UTC     deployed        web-dashboard-apd-0.1.0         1.16.0     


kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n atlanta-page-04 atlanta-page-apd -o json | jq -r '.spec.template.spec.containers[].image'


kubectl get deploy -n digi-locker-02 digi-locker-apd  -o json | jq -r '.spec.template.spec.containers[].image'




kubectl get deploy -n security-alpha-01 security-alpha-apd  -o json | jq -r '.spec.template.spec.containers[].image' 



    SECTION: APPLICATION DEPLOYMENT


    For this question, please set the context to cluster1 by running:

    kubectl config use-context cluster1



    On the cluster1, the team has installed multiple helm charts on a different namespace. By mistake, those deployed resources include one of the vulnerable images called kodekloud/click-counter:latest. Find out the release name and uninstall it.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster1


    In this task, we will use the helm commands and jq tool. Here are the steps: -


        Run the helm ls command with -A option to list the releases deployed on all the namespaces using helm.

    helm ls -A


        We will use the jq tool to extract the image name from the deployments.

    kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'


    Replace <NAMESPACE> with the namespace and <DEPLOYMENT-NAME> with the deployment name, which we get from the previous commands.

    After finding the kodekloud/click-counter:latest image, use the helm uninstall to remove the deployed chart that are using this vulnerable image.

    helm uninstall <RELEASE-NAME> -n <NAMESPACE> 
}



{ : Service Account :

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl command. Here are the steps: -


    Use the kubectl get command to list all the given resources: -

kubectl get po,deploy,sa,ns -n fusion-apd-x1df5


Here -n option stands for namespace, which is used to specify the namespace.

The above command will list all the resources from the fusion-apd-x1df5 namespace.

    Inspect the service account is used by the pods/deployment.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -oyaml



The deployment is using the default service account.


    Now, use the kubectl get command to retrieves the YAML definition of a deployment named robox-west-apd and save it into a file.

kubectl get deploy -n fusion-apd-x1df5 robox-west-apd -o yaml > <FILE-NAME>.yaml


    Open a VI editor. Make the necessary changes and save it. It should look like this: -

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    global-kgh: robox-west-apd
  name: robox-west-apd
  namespace: fusion-apd-x1df5
spec:
  replicas: 3
  selector:
    matchLabels:
      global-kgh: robox-west-apd
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        global-kgh: robox-west-apd
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: robox-container
      serviceAccountName: galaxy-apd-xb12 ( <--------- )


    Now, replace the resource with the following command:

kubectl replace -f <FILE-NAME>.yaml --force


The above command will delete the existing deployment and create a new one with changes in the given namespace.



}



{ : Broken :


    For this question, please set the context to cluster3 by running:

    kubectl config use-context cluster3


    One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


    After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.

    NOTE: - We have to perform this task on the cluster3-controlplane node.


    You can SSH into the cluster3 using ssh cluster3-controlplane command.

    info_outline
    Solution

    Run the following command to change the context: -

    kubectl config use-context cluster3


    In this task, we will use the kubectl and helm commands. Here are the steps: -


        Log in to the cluster3-controlplane node first and use the helm ls command to list all the releases installed using Helm in the Kubernetes cluster.

    helm ls -A


    Here -A or --all-namespaces option lists all the releases of all the namespaces.


        Identify the namespace where the resources get deployed.


        Use the helm repo ls command to list the helm repositories.

    helm repo ls 


        Now, update the helm repository with the following command: -

    helm repo update lvm-crystal-apd -n crystal-apd-ns


    The above command updates the local cache of available charts from the configured chart repositories.


        The helm search command searches for all the available charts in a specific Helm chart repository. In our case, it's the nginx helm chart.

    helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30



    The -l or --versions option is used to display information about all available chart versions.


        Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. Use the helm upgrade command as follows: -

    helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2


        After upgrading the chart version, you can verify it with the following command: -

    helm ls -n crystal-apd-ns


    Look under the CHART column for the chart version.


        Use the kubectl get command to check the replicas of the deployment: -

    kubectl get deploy -n crystal-apd-ns


    The available count 2 is under the AVAILABLE column.



	
}



{ : Update with Kubectl : 
		(
			[kubectl] [set] [image] [-n] [type the namespace here] [deploy(short for deployments.apps)] [deployment name (use k get {-n some-namespace-goes-here} deployment.apps) (and maybe add the namespace)] [container name goes here]=[container-image-version-with-a-tag (: is the tag with fun decimal numbers after it)]
		)

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl describe, kubectl get, kubectl set and kubectl scale commands. Here are the steps: -

    To check all the deployments in all the namespaces in the cluster2, we would have to run the following command:

kubectl get deployments -A


Inspect all the deployments.


    We can see that one of the deployment's names is results-apd and deployed on dashboard-apd namespace. Use the kubectl describe command to get detailed information of that deployment: -

kubectl describe -n dashboard-apd deploy results-apd


The output of the kubectl describe command will provide you with a detailed description of the deployment, including its name, namespace, creation time, labels, replicas, and the Docker image being used.


    In the previous command, we can see the container and image name under the Pod Template spec. Use the kubectl set command to update the image of that container as follows:

kubectl set image -n dashboard-apd deploy results-apd results-apd-container=nginx:1.23.3


After running the above command, Kubernetes will automatically update the results-apd-container container with the new image, and create a new replica of the resource with the updated image.
The old replica will be deleted once the new one is up and running.


    Now, SSH to the cluster2-controlplane node and use echo command to add this new image to a file at /root/records/new-image-records.txt: -

echo "nginx:1.23.3" > /root/records/new-image-records.txt


If the records directory is absent, use the mkdir command to create this directory.


    NOTE: - To exit from any node, type exit on the terminal or press CTRL + D.


    Now, run the kubectl scale command to scale the deployment to 4:

kubectl scale deployment -n dashboard-apd results-apd --replicas=4

[k] [scale] [deployment] [-n] ["some-namespace"] [deployment name] [--replicas]=[any number 0-9?]


Cross-verify the scaled deployment by using the kubectl get command:

kubectl get deployments,pods -n dashboard-apd



}


{

	Run the following command to change the context: -

kubectl config use-context cluster2


In this task, we will use the kubectl and helm commands. Here are the steps: -


    To check all the resources in all namespaces in the cluster2, we would have to run the following command:

kubectl get all -A


It will list all the available resources of all namespaces.


We can see that the resources have used prefix called garuda in the name.

    To list all of the releases on the garuda-apps-apd namespace. Run the following command as follows: -

helm ls -n garuda-apps-apd


We can see the release names of the web and security applications, and they are deployed on the garuda-apps-apd namespace.


    Write their release names by using the echo command as follows: -

echo "garuda-secret-apd,garuda-web-apd" > /root/apps-release-names.txt


    NOTE: - Make sure you have logged in student-node.


    On the same cluster, one more testing application is running on the testing-apd namespace, which we must delete because it is consuming resources.

helm ls -n testing-apd


helm uninstall -n testing-apd image-scanner 


}














{
	student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs 
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          7m57s

student-node ~ ➜  k get pods -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs -o yaml  > 1-pod-cm.yaml

# Using editor to modify the fields as per the requirements:
student-node ~ ➜  vim 1-pod-cm.yaml 

student-node ~ ➜  cat 1-pod-cm.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: ckad02-test-pod-aecs
  namespace: ckad02-mult-cm-cfg-aecs
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: ["/bin/sh", "-c", "while true; do env | egrep \"GREETINGS|WHO\"; sleep 10; done"]
      env:
        - name: GREETINGS
          valueFrom:
            configMapKeyRef:
              name: ckad02-config1-aecs
              key: greetings.how
        - name: WHO
          valueFrom:
            configMapKeyRef:
              name: ckad02-config2-aecs
              key: man
  restartPolicy: Always

student-node ~ ➜  k replace -f 1-pod-cm.yaml --force
pod "ckad02-test-pod-aecs" deleted
pod/ckad02-test-pod-aecs replaced

student-node ~ ➜  k get -f 1-pod-cm.yaml
NAME                   READY   STATUS    RESTARTS   AGE
ckad02-test-pod-aecs   1/1     Running   0          10s

student-node ~ ➜  k logs -n ckad02-mult-cm-cfg-aecs ckad02-test-pod-aecs | head -2
GREETINGS=HI
WHO=HANDSOME
}




{
	kubectl rollout history deployment -n blue-apd foundary-apd --revision=3
}



kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt











{

 Run the following command to change the context: -

kubectl config use-context cluster1


In this task, we will use the helm commands. Here are the steps: -


    Use the helm ls command to list the release deployed on the default namespace using helm.

helm ls -n default


    First, validate the helm chart by using the helm lint command: -

cd /root/

helm lint ./new-version


    Now, install the new version of the application by using the helm install command as follows: -

helm install --generate-name ./new-version


We haven't got any release name in the task, so we can generate the random name from the --generate-name option.

Finally, uninstall the old version of the application by using the helm uninstall command: -

helm uninstall webpage-server-01 -n default

	
}
